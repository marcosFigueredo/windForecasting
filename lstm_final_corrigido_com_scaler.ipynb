{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6148f558",
   "metadata": {},
   "source": [
    "## C√≥digo final consolidado: LSTM com HPO + linearidade + robustez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945d8224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Flatten, Concatenate, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from kerastuner.tuners import RandomSearch\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# === Fun√ß√£o para criar dataset multi-step ===\n",
    "def criar_dataset_multi_step(series, look_back=10, passo=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(series) - look_back - passo + 1):\n",
    "        X.append(series[i:i+look_back])\n",
    "        y.append(series[i+look_back+passo-1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# === Fun√ß√£o geradora de modelo com look_back fixado ===\n",
    "def get_build_model(look_back):\n",
    "    def build_model(hp):\n",
    "        inputs = Input(shape=(look_back, 1))\n",
    "        lstm_out = LSTM(\n",
    "            units=hp.Int('lstm_units', min_value=32, max_value=128, step=32),\n",
    "            return_sequences=False\n",
    "        )(inputs)\n",
    "\n",
    "        flattened = Flatten()(inputs)\n",
    "        normalized = BatchNormalization()(flattened)\n",
    "        linear_weights = Dense(\n",
    "            1, use_bias=False,\n",
    "            kernel_regularizer=l1_l2(l1=0.01, l2=0.01),\n",
    "            name=\"linear_combination\"\n",
    "        )(normalized)\n",
    "\n",
    "        combined = Concatenate()([lstm_out, linear_weights])\n",
    "        output = Dense(1)(combined)\n",
    "\n",
    "        model = Model(inputs, output)\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss=tf.keras.losses.Huber(delta=1.0),\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        return model\n",
    "    return build_model\n",
    "\n",
    "# === Loop para cada horizonte de previs√£o ===\n",
    "a3_df = pd.read_csv(\"A3_component.csv\")\n",
    "a3 = a3_df[\"A3\"].values.reshape(-1, 1)\n",
    "\n",
    "for passo in [1, 5, 7, 30]:\n",
    "    look_back = 5 if passo <= 5 else 10\n",
    "    print(f\"\\nüöÄ Treinando LSTM para t+{passo} com look_back={look_back}\")\n",
    "\n",
    "    # Normaliza√ß√£o\n",
    "    scaler = MinMaxScaler()\n",
    "    a3_scaled = scaler.fit_transform(a3)\n",
    "    joblib.dump(scaler, f\"scaler_A3_t{passo}.joblib\")\n",
    "\n",
    "    # Criar dataset\n",
    "    X, y = criar_dataset_multi_step(a3_scaled, look_back=look_back, passo=passo)\n",
    "\n",
    "    # Tuner\n",
    "    tuner = RandomSearch(\n",
    "        get_build_model(look_back),\n",
    "        objective='val_loss',\n",
    "        max_trials=5,\n",
    "        executions_per_trial=1,\n",
    "        directory='tuner_results',\n",
    "        project_name=f'lstm_t{passo}'\n",
    "    )\n",
    "\n",
    "    # Valida√ß√£o cruzada temporal\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        tuner.search(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_test, y_test),\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            callbacks=[EarlyStopping(patience=10, restore_best_weights=True)],\n",
    "            verbose=0\n",
    "        )\n",
    "        break  # usar apenas a 1¬™ divis√£o do TimeSeriesSplit\n",
    "\n",
    "    # Treinar melhor modelo\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "    best_model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=[EarlyStopping(patience=10, restore_best_weights=True)],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Salvar modelo\n",
    "    best_model.save(f\"lstm_a3_t{passo}.keras\")\n",
    "    print(f\"‚úÖ Modelo salvo: lstm_a3_t{passo}.keras\")\n",
    "\n",
    "\n",
    "import joblib\n",
    "joblib.dump(scaler_a3, 'scaler_a3.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
