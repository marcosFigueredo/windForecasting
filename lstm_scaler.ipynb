{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6148f558",
   "metadata": {},
   "source": [
    "## Código final consolidado: LSTM com HPO + linearidade + robustez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "945d8224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marco\\AppData\\Local\\Temp\\ipykernel_25868\\4086292502.py:10: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.tuners import RandomSearch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Treinando LSTM para t+1 com look_back=5\n",
      "Reloading Tuner from tuner_results\\lstm_t1\\tuner0.json\n",
      "WARNING:tensorflow:From C:\\Users\\marco\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marco\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 6.6009e-04 - mae: 0.0278 - val_loss: 4.5126e-04 - val_mae: 0.0249\n",
      "Epoch 2/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.8754e-04 - mae: 0.0213 - val_loss: 4.1534e-04 - val_mae: 0.0237\n",
      "Epoch 3/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.8140e-04 - mae: 0.0211 - val_loss: 3.7248e-04 - val_mae: 0.0225\n",
      "Epoch 4/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.2018e-04 - mae: 0.0229 - val_loss: 6.9821e-04 - val_mae: 0.0326\n",
      "Epoch 5/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.3398e-04 - mae: 0.0197 - val_loss: 2.9520e-04 - val_mae: 0.0197\n",
      "Epoch 6/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6993e-04 - mae: 0.0175 - val_loss: 2.8983e-04 - val_mae: 0.0194\n",
      "Epoch 7/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.5010e-04 - mae: 0.0172 - val_loss: 3.2448e-04 - val_mae: 0.0209\n",
      "Epoch 8/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.8964e-04 - mae: 0.0182 - val_loss: 3.2710e-04 - val_mae: 0.0210\n",
      "Epoch 9/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.6000e-04 - mae: 0.0211 - val_loss: 3.2944e-04 - val_mae: 0.0211\n",
      "Epoch 10/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.4004e-04 - mae: 0.0166 - val_loss: 2.7860e-04 - val_mae: 0.0193\n",
      "Epoch 11/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.3327e-04 - mae: 0.0164 - val_loss: 3.6551e-04 - val_mae: 0.0227\n",
      "Epoch 12/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.1695e-04 - mae: 0.0158 - val_loss: 3.5162e-04 - val_mae: 0.0226\n",
      "Epoch 13/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.1209e-04 - mae: 0.0190 - val_loss: 1.8966e-04 - val_mae: 0.0155\n",
      "Epoch 14/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.7898e-04 - mae: 0.0178 - val_loss: 1.8228e-04 - val_mae: 0.0151\n",
      "Epoch 15/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.8985e-04 - mae: 0.0143 - val_loss: 2.2604e-04 - val_mae: 0.0174\n",
      "Epoch 16/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.7431e-04 - mae: 0.0138 - val_loss: 1.6344e-04 - val_mae: 0.0142\n",
      "Epoch 17/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.3691e-04 - mae: 0.0167 - val_loss: 1.3667e-04 - val_mae: 0.0125\n",
      "Epoch 18/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.0674e-04 - mae: 0.0149 - val_loss: 1.5616e-04 - val_mae: 0.0138\n",
      "Epoch 19/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.8318e-04 - mae: 0.0143 - val_loss: 1.5864e-04 - val_mae: 0.0139\n",
      "Epoch 20/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.7607e-04 - mae: 0.0138 - val_loss: 2.2148e-04 - val_mae: 0.0165\n",
      "Epoch 21/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.4079e-04 - mae: 0.0201 - val_loss: 1.5684e-04 - val_mae: 0.0138\n",
      "Epoch 22/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.6863e-04 - mae: 0.0132 - val_loss: 1.8569e-04 - val_mae: 0.0156\n",
      "Epoch 23/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.6887e-04 - mae: 0.0136 - val_loss: 1.6384e-04 - val_mae: 0.0141\n",
      "Epoch 24/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.5453e-04 - mae: 0.0128 - val_loss: 1.3480e-04 - val_mae: 0.0125\n",
      "Epoch 25/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.5141e-04 - mae: 0.0124 - val_loss: 1.5639e-04 - val_mae: 0.0140\n",
      "Epoch 26/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.5363e-04 - mae: 0.0126 - val_loss: 1.0713e-04 - val_mae: 0.0109\n",
      "Epoch 27/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.5063e-04 - mae: 0.0122 - val_loss: 1.3353e-04 - val_mae: 0.0130\n",
      "Epoch 28/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.3992e-04 - mae: 0.0121 - val_loss: 1.0467e-04 - val_mae: 0.0106\n",
      "Epoch 29/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.3935e-04 - mae: 0.0120 - val_loss: 1.1313e-04 - val_mae: 0.0111\n",
      "Epoch 30/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.8657e-04 - mae: 0.0143 - val_loss: 1.0416e-04 - val_mae: 0.0105\n",
      "Epoch 31/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.7754e-04 - mae: 0.0135 - val_loss: 1.1662e-04 - val_mae: 0.0114\n",
      "Epoch 32/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.7118e-04 - mae: 0.0135 - val_loss: 1.1257e-04 - val_mae: 0.0117\n",
      "Epoch 33/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.3778e-04 - mae: 0.0118 - val_loss: 9.2096e-05 - val_mae: 0.0099\n",
      "Epoch 34/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.3479e-04 - mae: 0.0120 - val_loss: 1.8158e-04 - val_mae: 0.0162\n",
      "Epoch 35/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.7032e-04 - mae: 0.0142 - val_loss: 8.7394e-05 - val_mae: 0.0099\n",
      "Epoch 36/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.1607e-04 - mae: 0.0109 - val_loss: 1.3052e-04 - val_mae: 0.0132\n",
      "Epoch 37/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.3003e-04 - mae: 0.0121 - val_loss: 1.1271e-04 - val_mae: 0.0120\n",
      "Epoch 38/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0832e-04 - mae: 0.0104 - val_loss: 1.0308e-04 - val_mae: 0.0113\n",
      "Epoch 39/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.1534e-04 - mae: 0.0108 - val_loss: 1.2533e-04 - val_mae: 0.0117\n",
      "Epoch 40/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.0797e-04 - mae: 0.0161 - val_loss: 7.2972e-05 - val_mae: 0.0084\n",
      "Epoch 41/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0626e-04 - mae: 0.0103 - val_loss: 1.1063e-04 - val_mae: 0.0118\n",
      "Epoch 42/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.2181e-04 - mae: 0.0110 - val_loss: 1.0190e-04 - val_mae: 0.0115\n",
      "Epoch 43/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.1101e-04 - mae: 0.0104 - val_loss: 7.7579e-05 - val_mae: 0.0090\n",
      "Epoch 44/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0650e-04 - mae: 0.0105 - val_loss: 7.5089e-05 - val_mae: 0.0094\n",
      "Epoch 45/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.0213e-04 - mae: 0.0101 - val_loss: 6.5359e-05 - val_mae: 0.0082\n",
      "Epoch 46/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0073e-04 - mae: 0.0102 - val_loss: 1.5776e-04 - val_mae: 0.0153\n",
      "Epoch 47/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.1184e-04 - mae: 0.0110 - val_loss: 9.4095e-05 - val_mae: 0.0108\n",
      "Epoch 48/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.1961e-04 - mae: 0.0112 - val_loss: 7.0238e-05 - val_mae: 0.0085\n",
      "Epoch 49/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0788e-04 - mae: 0.0105 - val_loss: 7.8720e-05 - val_mae: 0.0097\n",
      "Epoch 50/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.9440e-05 - mae: 0.0102 - val_loss: 1.8144e-04 - val_mae: 0.0166\n",
      "Epoch 51/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.3241e-04 - mae: 0.0124 - val_loss: 5.8910e-05 - val_mae: 0.0076\n",
      "Epoch 52/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.7838e-05 - mae: 0.0096 - val_loss: 1.7133e-04 - val_mae: 0.0163\n",
      "Epoch 53/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.2073e-04 - mae: 0.0120 - val_loss: 9.7697e-05 - val_mae: 0.0102\n",
      "Epoch 54/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.3180e-04 - mae: 0.0124 - val_loss: 6.5128e-05 - val_mae: 0.0084\n",
      "Epoch 55/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.7637e-05 - mae: 0.0095 - val_loss: 5.7025e-05 - val_mae: 0.0075\n",
      "Epoch 56/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.8503e-05 - mae: 0.0104 - val_loss: 5.2099e-05 - val_mae: 0.0072\n",
      "Epoch 57/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.8000e-05 - mae: 0.0092 - val_loss: 4.9501e-05 - val_mae: 0.0067\n",
      "Epoch 58/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.4511e-05 - mae: 0.0096 - val_loss: 1.0181e-04 - val_mae: 0.0119\n",
      "Epoch 59/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.7318e-05 - mae: 0.0106 - val_loss: 5.9605e-05 - val_mae: 0.0085\n",
      "Epoch 60/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.1323e-05 - mae: 0.0086 - val_loss: 5.5714e-05 - val_mae: 0.0076\n",
      "Epoch 61/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.2301e-05 - mae: 0.0096 - val_loss: 4.9942e-05 - val_mae: 0.0075\n",
      "Epoch 62/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.3200e-05 - mae: 0.0087 - val_loss: 5.3753e-05 - val_mae: 0.0073\n",
      "Epoch 63/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.1085e-05 - mae: 0.0095 - val_loss: 1.0776e-04 - val_mae: 0.0123\n",
      "Epoch 64/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.0979e-05 - mae: 0.0102 - val_loss: 4.4776e-05 - val_mae: 0.0064\n",
      "Epoch 65/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.1321e-05 - mae: 0.0086 - val_loss: 7.7499e-05 - val_mae: 0.0102\n",
      "Epoch 66/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.7632e-05 - mae: 0.0092 - val_loss: 5.3671e-05 - val_mae: 0.0075\n",
      "Epoch 67/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.0363e-05 - mae: 0.0094 - val_loss: 5.9022e-05 - val_mae: 0.0078\n",
      "Epoch 68/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.0122e-05 - mae: 0.0094 - val_loss: 1.1610e-04 - val_mae: 0.0129\n",
      "Epoch 69/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.5554e-05 - mae: 0.0105 - val_loss: 4.4268e-05 - val_mae: 0.0064\n",
      "Epoch 70/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.4194e-05 - mae: 0.0080 - val_loss: 4.4116e-05 - val_mae: 0.0064\n",
      "Epoch 71/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.7874e-05 - mae: 0.0082 - val_loss: 6.6722e-05 - val_mae: 0.0091\n",
      "Epoch 72/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.4554e-05 - mae: 0.0083 - val_loss: 5.6073e-05 - val_mae: 0.0078\n",
      "Epoch 73/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.2368e-04 - mae: 0.0122 - val_loss: 6.3339e-05 - val_mae: 0.0086\n",
      "Epoch 74/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.7655e-05 - mae: 0.0090 - val_loss: 5.1732e-05 - val_mae: 0.0075\n",
      "Epoch 75/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.1324e-05 - mae: 0.0085 - val_loss: 9.6065e-05 - val_mae: 0.0116\n",
      "Epoch 76/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.7025e-05 - mae: 0.0098 - val_loss: 4.8924e-05 - val_mae: 0.0070\n",
      "Epoch 77/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.7492e-05 - mae: 0.0083 - val_loss: 5.3861e-05 - val_mae: 0.0079\n",
      "Epoch 78/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.5587e-05 - mae: 0.0081 - val_loss: 4.5481e-05 - val_mae: 0.0068\n",
      "Epoch 79/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.0651e-05 - mae: 0.0084 - val_loss: 1.8030e-04 - val_mae: 0.0171\n",
      "Epoch 80/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.0334e-04 - mae: 0.0107 - val_loss: 6.4792e-05 - val_mae: 0.0088\n",
      "✅ Modelo salvo: lstm_a3_t1.keras\n",
      "\n",
      "🚀 Treinando LSTM para t+5 com look_back=5\n",
      "Reloading Tuner from tuner_results\\lstm_t5\\tuner0.json\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marco\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0048 - mae: 0.0757 - val_loss: 0.0054 - val_mae: 0.0847\n",
      "Epoch 2/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0045 - mae: 0.0731 - val_loss: 0.0061 - val_mae: 0.0908\n",
      "Epoch 3/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0042 - mae: 0.0708 - val_loss: 0.0066 - val_mae: 0.0953\n",
      "Epoch 4/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0048 - mae: 0.0760 - val_loss: 0.0071 - val_mae: 0.0996\n",
      "Epoch 5/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0047 - mae: 0.0757 - val_loss: 0.0051 - val_mae: 0.0823\n",
      "Epoch 6/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0042 - mae: 0.0706 - val_loss: 0.0053 - val_mae: 0.0839\n",
      "Epoch 7/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0043 - mae: 0.0712 - val_loss: 0.0053 - val_mae: 0.0847\n",
      "Epoch 8/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0041 - mae: 0.0699 - val_loss: 0.0057 - val_mae: 0.0873\n",
      "Epoch 9/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0047 - mae: 0.0737 - val_loss: 0.0049 - val_mae: 0.0807\n",
      "Epoch 10/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0040 - mae: 0.0685 - val_loss: 0.0042 - val_mae: 0.0748\n",
      "Epoch 11/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0043 - mae: 0.0708 - val_loss: 0.0047 - val_mae: 0.0795\n",
      "Epoch 12/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0046 - mae: 0.0751 - val_loss: 0.0045 - val_mae: 0.0770\n",
      "Epoch 13/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0040 - mae: 0.0690 - val_loss: 0.0056 - val_mae: 0.0871\n",
      "Epoch 14/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0041 - mae: 0.0716 - val_loss: 0.0052 - val_mae: 0.0830\n",
      "Epoch 15/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0039 - mae: 0.0686 - val_loss: 0.0048 - val_mae: 0.0799\n",
      "Epoch 16/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0043 - mae: 0.0727 - val_loss: 0.0037 - val_mae: 0.0690\n",
      "Epoch 17/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0046 - mae: 0.0748 - val_loss: 0.0043 - val_mae: 0.0758\n",
      "Epoch 18/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0040 - mae: 0.0698 - val_loss: 0.0043 - val_mae: 0.0762\n",
      "Epoch 19/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0037 - mae: 0.0660 - val_loss: 0.0041 - val_mae: 0.0739\n",
      "Epoch 20/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0037 - mae: 0.0664 - val_loss: 0.0048 - val_mae: 0.0811\n",
      "Epoch 21/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0039 - mae: 0.0698 - val_loss: 0.0035 - val_mae: 0.0683\n",
      "Epoch 22/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0034 - mae: 0.0639 - val_loss: 0.0046 - val_mae: 0.0794\n",
      "Epoch 23/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0037 - mae: 0.0666 - val_loss: 0.0047 - val_mae: 0.0800\n",
      "Epoch 24/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0034 - mae: 0.0639 - val_loss: 0.0044 - val_mae: 0.0785\n",
      "Epoch 25/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0037 - mae: 0.0655 - val_loss: 0.0037 - val_mae: 0.0713\n",
      "Epoch 26/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0044 - mae: 0.0736 - val_loss: 0.0037 - val_mae: 0.0702\n",
      "Epoch 27/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0035 - mae: 0.0637 - val_loss: 0.0029 - val_mae: 0.0611\n",
      "Epoch 28/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0034 - mae: 0.0639 - val_loss: 0.0033 - val_mae: 0.0660\n",
      "Epoch 29/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0036 - mae: 0.0644 - val_loss: 0.0031 - val_mae: 0.0640\n",
      "Epoch 30/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0033 - mae: 0.0625 - val_loss: 0.0030 - val_mae: 0.0627\n",
      "Epoch 31/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0033 - mae: 0.0628 - val_loss: 0.0024 - val_mae: 0.0540\n",
      "Epoch 32/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0034 - mae: 0.0639 - val_loss: 0.0024 - val_mae: 0.0554\n",
      "Epoch 33/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0035 - mae: 0.0640 - val_loss: 0.0034 - val_mae: 0.0677\n",
      "Epoch 34/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0032 - mae: 0.0631 - val_loss: 0.0036 - val_mae: 0.0709\n",
      "Epoch 35/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0037 - mae: 0.0654 - val_loss: 0.0035 - val_mae: 0.0694\n",
      "Epoch 36/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0032 - mae: 0.0624 - val_loss: 0.0038 - val_mae: 0.0718\n",
      "Epoch 37/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0031 - mae: 0.0608 - val_loss: 0.0027 - val_mae: 0.0593\n",
      "Epoch 38/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0031 - mae: 0.0595 - val_loss: 0.0030 - val_mae: 0.0628\n",
      "Epoch 39/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0030 - mae: 0.0611 - val_loss: 0.0026 - val_mae: 0.0584\n",
      "Epoch 40/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0037 - mae: 0.0662 - val_loss: 0.0022 - val_mae: 0.0510\n",
      "Epoch 41/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0034 - mae: 0.0631 - val_loss: 0.0049 - val_mae: 0.0835\n",
      "Epoch 42/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0033 - mae: 0.0646 - val_loss: 0.0030 - val_mae: 0.0626\n",
      "Epoch 43/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0032 - mae: 0.0624 - val_loss: 0.0029 - val_mae: 0.0620\n",
      "Epoch 44/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0031 - mae: 0.0609 - val_loss: 0.0041 - val_mae: 0.0761\n",
      "Epoch 45/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0034 - mae: 0.0640 - val_loss: 0.0058 - val_mae: 0.0930\n",
      "Epoch 46/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0035 - mae: 0.0655 - val_loss: 0.0026 - val_mae: 0.0583\n",
      "Epoch 47/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0033 - mae: 0.0623 - val_loss: 0.0029 - val_mae: 0.0622\n",
      "Epoch 48/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0030 - mae: 0.0594 - val_loss: 0.0032 - val_mae: 0.0657\n",
      "Epoch 49/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0031 - mae: 0.0607 - val_loss: 0.0039 - val_mae: 0.0736\n",
      "Epoch 50/100\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0030 - mae: 0.0602 - val_loss: 0.0025 - val_mae: 0.0560\n",
      "✅ Modelo salvo: lstm_a3_t5.keras\n",
      "\n",
      "🚀 Treinando LSTM para t+7 com look_back=10\n",
      "Reloading Tuner from tuner_results\\lstm_t7\\tuner0.json\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marco\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - loss: 0.0065 - mae: 0.0882 - val_loss: 0.0056 - val_mae: 0.0851\n",
      "Epoch 2/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0058 - mae: 0.0830 - val_loss: 0.0060 - val_mae: 0.0880\n",
      "Epoch 3/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0064 - mae: 0.0872 - val_loss: 0.0065 - val_mae: 0.0938\n",
      "Epoch 4/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0063 - mae: 0.0875 - val_loss: 0.0049 - val_mae: 0.0789\n",
      "Epoch 5/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0064 - mae: 0.0875 - val_loss: 0.0056 - val_mae: 0.0857\n",
      "Epoch 6/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0057 - mae: 0.0813 - val_loss: 0.0044 - val_mae: 0.0737\n",
      "Epoch 7/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0057 - mae: 0.0821 - val_loss: 0.0051 - val_mae: 0.0805\n",
      "Epoch 8/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0060 - mae: 0.0851 - val_loss: 0.0053 - val_mae: 0.0833\n",
      "Epoch 9/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0055 - mae: 0.0801 - val_loss: 0.0057 - val_mae: 0.0864\n",
      "Epoch 10/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0058 - mae: 0.0833 - val_loss: 0.0053 - val_mae: 0.0828\n",
      "Epoch 11/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0053 - mae: 0.0776 - val_loss: 0.0058 - val_mae: 0.0875\n",
      "Epoch 12/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0056 - mae: 0.0805 - val_loss: 0.0044 - val_mae: 0.0738\n",
      "Epoch 13/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0057 - mae: 0.0806 - val_loss: 0.0049 - val_mae: 0.0783\n",
      "Epoch 14/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0056 - mae: 0.0802 - val_loss: 0.0048 - val_mae: 0.0780\n",
      "Epoch 15/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0062 - mae: 0.0843 - val_loss: 0.0043 - val_mae: 0.0719\n",
      "Epoch 16/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0063 - mae: 0.0845 - val_loss: 0.0050 - val_mae: 0.0805\n",
      "Epoch 17/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0054 - mae: 0.0790 - val_loss: 0.0041 - val_mae: 0.0699\n",
      "Epoch 18/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0055 - mae: 0.0792 - val_loss: 0.0043 - val_mae: 0.0721\n",
      "Epoch 19/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0054 - mae: 0.0796 - val_loss: 0.0048 - val_mae: 0.0775\n",
      "Epoch 20/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0052 - mae: 0.0796 - val_loss: 0.0054 - val_mae: 0.0828\n",
      "Epoch 21/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0052 - mae: 0.0779 - val_loss: 0.0055 - val_mae: 0.0841\n",
      "Epoch 22/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0051 - mae: 0.0771 - val_loss: 0.0041 - val_mae: 0.0680\n",
      "Epoch 23/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0054 - mae: 0.0784 - val_loss: 0.0058 - val_mae: 0.0867\n",
      "Epoch 24/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0056 - mae: 0.0822 - val_loss: 0.0042 - val_mae: 0.0695\n",
      "Epoch 25/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0052 - mae: 0.0775 - val_loss: 0.0048 - val_mae: 0.0772\n",
      "Epoch 26/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0052 - mae: 0.0775 - val_loss: 0.0045 - val_mae: 0.0744\n",
      "Epoch 27/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0052 - mae: 0.0785 - val_loss: 0.0047 - val_mae: 0.0760\n",
      "Epoch 28/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0056 - mae: 0.0800 - val_loss: 0.0038 - val_mae: 0.0658\n",
      "Epoch 29/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0053 - mae: 0.0777 - val_loss: 0.0046 - val_mae: 0.0749\n",
      "Epoch 30/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0052 - mae: 0.0785 - val_loss: 0.0048 - val_mae: 0.0769\n",
      "Epoch 31/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0052 - mae: 0.0777 - val_loss: 0.0044 - val_mae: 0.0724\n",
      "Epoch 32/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0053 - mae: 0.0807 - val_loss: 0.0047 - val_mae: 0.0748\n",
      "Epoch 33/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0049 - mae: 0.0763 - val_loss: 0.0060 - val_mae: 0.0888\n",
      "Epoch 34/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0055 - mae: 0.0813 - val_loss: 0.0040 - val_mae: 0.0685\n",
      "Epoch 35/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0050 - mae: 0.0768 - val_loss: 0.0046 - val_mae: 0.0734\n",
      "Epoch 36/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0047 - mae: 0.0745 - val_loss: 0.0037 - val_mae: 0.0646\n",
      "Epoch 37/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0047 - mae: 0.0734 - val_loss: 0.0037 - val_mae: 0.0642\n",
      "Epoch 38/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0052 - mae: 0.0779 - val_loss: 0.0052 - val_mae: 0.0814\n",
      "Epoch 39/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0051 - mae: 0.0773 - val_loss: 0.0066 - val_mae: 0.0921\n",
      "Epoch 40/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0054 - mae: 0.0809 - val_loss: 0.0049 - val_mae: 0.0771\n",
      "Epoch 41/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0048 - mae: 0.0747 - val_loss: 0.0043 - val_mae: 0.0726\n",
      "Epoch 42/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0049 - mae: 0.0773 - val_loss: 0.0040 - val_mae: 0.0682\n",
      "Epoch 43/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0052 - mae: 0.0783 - val_loss: 0.0050 - val_mae: 0.0792\n",
      "Epoch 44/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0051 - mae: 0.0762 - val_loss: 0.0047 - val_mae: 0.0728\n",
      "Epoch 45/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0049 - mae: 0.0749 - val_loss: 0.0046 - val_mae: 0.0739\n",
      "Epoch 46/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0049 - mae: 0.0746 - val_loss: 0.0064 - val_mae: 0.0904\n",
      "✅ Modelo salvo: lstm_a3_t7.keras\n",
      "\n",
      "🚀 Treinando LSTM para t+30 com look_back=10\n",
      "Reloading Tuner from tuner_results\\lstm_t30\\tuner0.json\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marco\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - loss: 0.0091 - mae: 0.1054 - val_loss: 0.0141 - val_mae: 0.1406\n",
      "Epoch 2/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0087 - mae: 0.1029 - val_loss: 0.0155 - val_mae: 0.1487\n",
      "Epoch 3/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0089 - mae: 0.1046 - val_loss: 0.0156 - val_mae: 0.1488\n",
      "Epoch 4/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0086 - mae: 0.1031 - val_loss: 0.0127 - val_mae: 0.1333\n",
      "Epoch 5/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0087 - mae: 0.1024 - val_loss: 0.0136 - val_mae: 0.1378\n",
      "Epoch 6/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0093 - mae: 0.1051 - val_loss: 0.0155 - val_mae: 0.1476\n",
      "Epoch 7/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0088 - mae: 0.1041 - val_loss: 0.0151 - val_mae: 0.1457\n",
      "Epoch 8/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0080 - mae: 0.0994 - val_loss: 0.0148 - val_mae: 0.1442\n",
      "Epoch 9/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0093 - mae: 0.1063 - val_loss: 0.0144 - val_mae: 0.1419\n",
      "Epoch 10/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0085 - mae: 0.1020 - val_loss: 0.0141 - val_mae: 0.1403\n",
      "Epoch 11/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0090 - mae: 0.1045 - val_loss: 0.0143 - val_mae: 0.1418\n",
      "Epoch 12/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0083 - mae: 0.0997 - val_loss: 0.0161 - val_mae: 0.1514\n",
      "Epoch 13/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0095 - mae: 0.1081 - val_loss: 0.0156 - val_mae: 0.1486\n",
      "Epoch 14/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0087 - mae: 0.1029 - val_loss: 0.0132 - val_mae: 0.1357\n",
      "✅ Modelo salvo: lstm_a3_t30.keras\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Flatten, Concatenate, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from kerastuner.tuners import RandomSearch\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# === Função para criar dataset multi-step ===\n",
    "def criar_dataset_multi_step(series, look_back=10, passo=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(series) - look_back - passo + 1):\n",
    "        X.append(series[i:i+look_back])\n",
    "        y.append(series[i+look_back+passo-1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# === Função geradora de modelo com look_back fixado ===\n",
    "def get_build_model(look_back):\n",
    "    def build_model(hp):\n",
    "        inputs = Input(shape=(look_back, 1))\n",
    "        lstm_out = LSTM(\n",
    "            units=hp.Int('lstm_units', min_value=32, max_value=128, step=32),\n",
    "            return_sequences=False\n",
    "        )(inputs)\n",
    "\n",
    "        flattened = Flatten()(inputs)\n",
    "        normalized = BatchNormalization()(flattened)\n",
    "        linear_weights = Dense(\n",
    "            1, use_bias=False,\n",
    "            kernel_regularizer=l1_l2(l1=0.01, l2=0.01),\n",
    "            name=\"linear_combination\"\n",
    "        )(normalized)\n",
    "\n",
    "        combined = Concatenate()([lstm_out, linear_weights])\n",
    "        output = Dense(1)(combined)\n",
    "\n",
    "        model = Model(inputs, output)\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss=tf.keras.losses.Huber(delta=1.0),\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        return model\n",
    "    return build_model\n",
    "\n",
    "# === Loop para cada horizonte de previsão ===\n",
    "a3_df = pd.read_csv(\"A3_component.csv\")\n",
    "a3 = a3_df[\"A3\"].values.reshape(-1, 1)\n",
    "\n",
    "for passo in [1, 5, 7, 30]:\n",
    "    look_back = 5 if passo <= 5 else 10\n",
    "    print(f\"\\n🚀 Treinando LSTM para t+{passo} com look_back={look_back}\")\n",
    "\n",
    "    # Normalização\n",
    "    scaler = MinMaxScaler()\n",
    "    a3_scaled = scaler.fit_transform(a3)\n",
    "    joblib.dump(scaler, f\"scaler_A3_t{passo}.joblib\")\n",
    "\n",
    "    # Criar dataset\n",
    "    X, y = criar_dataset_multi_step(a3_scaled, look_back=look_back, passo=passo)\n",
    "\n",
    "    # Tuner\n",
    "    tuner = RandomSearch(\n",
    "        get_build_model(look_back),\n",
    "        objective='val_loss',\n",
    "        max_trials=5,\n",
    "        executions_per_trial=1,\n",
    "        directory='tuner_results',\n",
    "        project_name=f'lstm_t{passo}'\n",
    "    )\n",
    "\n",
    "    # Validação cruzada temporal\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        tuner.search(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_test, y_test),\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            callbacks=[EarlyStopping(patience=10, restore_best_weights=True)],\n",
    "            verbose=0\n",
    "        )\n",
    "        break  # usar apenas a 1ª divisão do TimeSeriesSplit\n",
    "\n",
    "    # Treinar melhor modelo\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "    best_model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=[EarlyStopping(patience=10, restore_best_weights=True)],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Salvar modelo\n",
    "    best_model.save(f\"lstm_a3_t{passo}.keras\")\n",
    "    print(f\"✅ Modelo salvo: lstm_a3_t{passo}.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484c0048",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
