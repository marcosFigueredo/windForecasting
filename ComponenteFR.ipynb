{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a3fcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import json\n",
    "import time\n",
    "\n",
    "def criar_dataset_multi_step(series, look_back=10, passo=1, adicionar_features=True):\n",
    "    X, y = [], []\n",
    "    for i in range(len(series) - look_back - passo + 1):\n",
    "        # Extrair a sequ√™ncia base\n",
    "        sequencia = series[i:i + look_back].flatten()\n",
    "        \n",
    "        if adicionar_features:\n",
    "            # Adicionar caracter√≠sticas para detec√ß√£o de picos\n",
    "            media_movel = np.mean(sequencia)\n",
    "            std_movel = np.std(sequencia)\n",
    "            max_local = np.max(sequencia)\n",
    "            min_local = np.min(sequencia)\n",
    "            amplitude = max_local - min_local\n",
    "            tendencia = sequencia[-1] - sequencia[0]\n",
    "            \n",
    "            # Derivada (taxa de mudan√ßa)\n",
    "            derivada = np.diff(sequencia)\n",
    "            derivada_mean = np.mean(derivada) if len(derivada) > 0 else 0\n",
    "            derivada_std = np.std(derivada) if len(derivada) > 0 else 0\n",
    "            \n",
    "            # Segunda derivada (acelera√ß√£o)\n",
    "            segunda_derivada = np.diff(derivada) if len(derivada) > 1 else np.array([0])\n",
    "            segunda_derivada_mean = np.mean(segunda_derivada) if len(segunda_derivada) > 0 else 0\n",
    "            \n",
    "            # Features adicionais\n",
    "            features = np.array([media_movel, std_movel, max_local, min_local, \n",
    "                               amplitude, tendencia, derivada_mean, derivada_std, \n",
    "                               segunda_derivada_mean])\n",
    "            \n",
    "            # Combinar features originais com as novas features\n",
    "            X.append(np.concatenate([sequencia, features]))\n",
    "        else:\n",
    "            X.append(sequencia)\n",
    "            \n",
    "        y.append(series[i + look_back + passo - 1])\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def treinar_rf_para_componentes(caminhos_componentes, passos=[1], rf_params=None, look_back=None):\n",
    "    \"\"\"\n",
    "    Treina modelos RandomForest para componentes wavelet.\n",
    "    \n",
    "    Args:\n",
    "        caminhos_componentes: Dicion√°rio com nome do componente e caminho do arquivo CSV\n",
    "        passos: Lista de horizontes de previs√£o (t+n)\n",
    "        rf_params: Dicion√°rio com par√¢metros para o RandomForestRegressor\n",
    "        look_back: Valor de look_back para a janela de hist√≥rico. Se None, ser√° calculado com base no passo.\n",
    "    \n",
    "    Returns:\n",
    "        Dicion√°rio com metadados dos modelos treinados\n",
    "    \"\"\"\n",
    "    # Tempo de in√≠cio\n",
    "    tempo_inicio = time.time()\n",
    "    \n",
    "    # Par√¢metros padr√£o do RandomForest se n√£o forem especificados\n",
    "    if rf_params is None:\n",
    "        rf_params = {\n",
    "            'n_estimators': 200,\n",
    "            'max_depth': 15,\n",
    "            'min_samples_split': 5,\n",
    "            'min_samples_leaf': 1,\n",
    "            'max_features': 'sqrt',\n",
    "            'bootstrap': True,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "    \n",
    "    # Criar diret√≥rios necess√°rios\n",
    "    os.makedirs(\"modelosRF\", exist_ok=True)\n",
    "    os.makedirs(\"scalersRF\", exist_ok=True)\n",
    "    os.makedirs(\"configsRF\", exist_ok=True)\n",
    "    \n",
    "    # Dicion√°rio para armazenar metadados de todos os modelos\n",
    "    metadados_modelos = {}\n",
    "    \n",
    "    # Exibir configura√ß√£o\n",
    "    print(\"\\n‚öôÔ∏è Configura√ß√£o de Treinamento:\")\n",
    "    print(f\"   - Componentes: {list(caminhos_componentes.keys())}\")\n",
    "    print(f\"   - Horizontes de previs√£o: {passos}\")\n",
    "    print(f\"   - Par√¢metros RF:\")\n",
    "    for param, valor in rf_params.items():\n",
    "        print(f\"      - {param}: {valor}\")\n",
    "    \n",
    "    # Percorrer cada componente\n",
    "    for comp, caminho_csv in caminhos_componentes.items():\n",
    "        tempo_componente = time.time()\n",
    "        print(f\"\\nüîç Processando componente: {comp.upper()}\")\n",
    "        \n",
    "        try:\n",
    "            # Carregar dados\n",
    "            df = pd.read_csv(caminho_csv)\n",
    "            \n",
    "            # Garantir que o componente exista no DataFrame\n",
    "            coluna_componente = comp.upper()\n",
    "            if coluna_componente not in df.columns:\n",
    "                print(f\"‚ö†Ô∏è Coluna {coluna_componente} n√£o encontrada em {caminho_csv}\")\n",
    "                continue\n",
    "                \n",
    "            serie = df[coluna_componente].values.reshape(-1, 1)\n",
    "            \n",
    "            # Limpeza e interpola√ß√£o de dados faltantes\n",
    "            serie = pd.Series(serie.flatten()).interpolate().ffill().bfill().values.reshape(-1, 1)\n",
    "            \n",
    "            # Verificar se h√° valores infinitos ou NaN\n",
    "            if np.isnan(serie).any() or np.isinf(serie).any():\n",
    "                print(f\"‚ö†Ô∏è Valores NaN ou infinitos encontrados em {comp}. Aplicando limpeza...\")\n",
    "                serie = np.nan_to_num(serie, nan=np.nanmean(serie), posinf=np.nanmax(serie), neginf=np.nanmin(serie))\n",
    "            \n",
    "            # Tamanho do dataset\n",
    "            print(f\"   - Tamanho do dataset: {len(serie)} pontos\")\n",
    "            \n",
    "            # Metadados espec√≠ficos do componente\n",
    "            metadados_modelos[comp] = {}\n",
    "            \n",
    "            # Treinar para cada horizonte de previs√£o\n",
    "            for passo in passos:\n",
    "                tempo_passo = time.time()\n",
    "                \n",
    "                # Calcular look_back se n√£o foi especificado\n",
    "                lookback_atual = look_back if look_back is not None else max(10, passo // 2)\n",
    "                \n",
    "                print(f\"\\nüå≤ Treinando RandomForest para {comp.upper()} | t+{passo} (look_back={lookback_atual})\")\n",
    "                \n",
    "                # Usar RobustScaler para lidar melhor com outliers\n",
    "                scaler = RobustScaler()\n",
    "                serie_scaled = scaler.fit_transform(serie)\n",
    "                \n",
    "                # Salvar scaler\n",
    "                scaler_filename = f\"scalersRF/scaler_{comp}_t{passo}.joblib\"\n",
    "                joblib.dump(scaler, scaler_filename)\n",
    "                \n",
    "                # Criar dataset com features adicionais\n",
    "                X, y = criar_dataset_multi_step(serie_scaled, look_back=lookback_atual, passo=passo, adicionar_features=True)\n",
    "                \n",
    "                # Informa√ß√µes sobre o dataset\n",
    "                print(f\"   - Dataset: {X.shape[0]} amostras, {X.shape[1]} features\")\n",
    "                \n",
    "                # Divis√£o treino/teste\n",
    "                split = int(0.8 * len(X))\n",
    "                X_train = X[:split]\n",
    "                y_train = y[:split]\n",
    "                \n",
    "                # Dicion√°rio para armazenar metadados do modelo\n",
    "                modelo_info = {\n",
    "                    'componente': comp,\n",
    "                    'passo': passo,\n",
    "                    'look_back': lookback_atual,\n",
    "                    'num_features': X.shape[1],\n",
    "                    'num_amostras_treino': len(X_train),\n",
    "                    'data_treinamento': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'usa_features_adicionais': True,\n",
    "                    'parametros': rf_params\n",
    "                }\n",
    "                \n",
    "                # Criar e treinar modelo com os par√¢metros fornecidos\n",
    "                print(f\"   - Iniciando treinamento...\")\n",
    "                model = RandomForestRegressor(**rf_params)\n",
    "                model.fit(X_train, y_train.ravel())\n",
    "                \n",
    "                # Se OOB score estiver dispon√≠vel, salvar\n",
    "                if hasattr(model, 'oob_score_'):\n",
    "                    modelo_info['oob_score'] = float(model.oob_score_)\n",
    "                    print(f\"   - OOB Score: {model.oob_score_:.4f}\")\n",
    "                \n",
    "                # Calcular e mostrar import√¢ncia das features\n",
    "                if len(model.feature_importances_) == X.shape[1]:\n",
    "                    # N√∫mero base de features originais\n",
    "                    num_features_orig = lookback_atual\n",
    "                    \n",
    "                    # Import√¢ncia das features originais (valores passados)\n",
    "                    imp_features_orig = model.feature_importances_[:num_features_orig].sum()\n",
    "                    \n",
    "                    # Import√¢ncia das features engineered\n",
    "                    imp_features_eng = model.feature_importances_[num_features_orig:].sum()\n",
    "                    \n",
    "                    print(f\"   - Import√¢ncia das features:\")\n",
    "                    print(f\"      - Features originais: {imp_features_orig:.4f} ({imp_features_orig*100:.1f}%)\")\n",
    "                    print(f\"      - Features engineered: {imp_features_eng:.4f} ({imp_features_eng*100:.1f}%)\")\n",
    "                    \n",
    "                    # Top 5 features mais importantes\n",
    "                    top_indices = np.argsort(model.feature_importances_)[-5:][::-1]\n",
    "                    \n",
    "                    # Nomes das features\n",
    "                    feature_names = [f\"t-{i+1}\" for i in range(lookback_atual)]\n",
    "                    feature_names.extend([\"m√©dia\", \"std\", \"max\", \"min\", \"amplitude\", \n",
    "                                          \"tend√™ncia\", \"derivada_m√©dia\", \"derivada_std\", \n",
    "                                          \"2¬™_derivada_m√©dia\"])\n",
    "                    \n",
    "                    print(f\"      - Top 5 features:\")\n",
    "                    for i, idx in enumerate(top_indices):\n",
    "                        if idx < len(feature_names):\n",
    "                            print(f\"         {i+1}. {feature_names[idx]}: {model.feature_importances_[idx]:.4f}\")\n",
    "                        else:\n",
    "                            print(f\"         {i+1}. Feature {idx}: {model.feature_importances_[idx]:.4f}\")\n",
    "                    \n",
    "                    # Salvar import√¢ncia das features no modelo_info\n",
    "                    modelo_info['importancia_features'] = {\n",
    "                        'features_originais': float(imp_features_orig),\n",
    "                        'features_engineered': float(imp_features_eng),\n",
    "                        'top_5_indices': top_indices.tolist(),\n",
    "                        'top_5_valores': model.feature_importances_[top_indices].tolist(),\n",
    "                        'todas_importancias': model.feature_importances_.tolist()\n",
    "                    }\n",
    "                \n",
    "                # Salvar modelo\n",
    "                modelo_filename = f\"modelosRF/rf_{comp}_t{passo}.joblib\"\n",
    "                joblib.dump(model, modelo_filename)\n",
    "                \n",
    "                # Salvar metadados do modelo\n",
    "                config_filename = f\"configsRF/config_{comp}_t{passo}.json\"\n",
    "                with open(config_filename, 'w') as f:\n",
    "                    json.dump(modelo_info, f, indent=4)\n",
    "                \n",
    "                # Adicionar informa√ß√£o ao dicion√°rio global\n",
    "                metadados_modelos[comp][f't{passo}'] = modelo_info\n",
    "                \n",
    "                # Tempo de treinamento para este horizonte\n",
    "                tempo_passo_fim = time.time() - tempo_passo\n",
    "                print(f\"‚úÖ Modelo salvo: {modelo_filename}\")\n",
    "                print(f\"üìù Configura√ß√£o salva: {config_filename}\")\n",
    "                print(f\"‚è±Ô∏è Tempo de treinamento para t+{passo}: {tempo_passo_fim:.2f} segundos\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao processar componente {comp}: {str(e)}\")\n",
    "        \n",
    "        # Tempo para este componente\n",
    "        tempo_componente_fim = time.time() - tempo_componente\n",
    "        print(f\"‚è±Ô∏è Tempo total para componente {comp.upper()}: {tempo_componente_fim:.2f} segundos\")\n",
    "    \n",
    "    # Salvar metadados globais\n",
    "    with open(\"configsRF/metadados_global.json\", 'w') as f:\n",
    "        json.dump(metadados_modelos, f, indent=4)\n",
    "    \n",
    "    print(f\"\\nüìä Metadados de todos os modelos salvos em: configsRF/metadados_global.json\")\n",
    "    \n",
    "    # Tempo total\n",
    "    tempo_total = time.time() - tempo_inicio\n",
    "    minutos = int(tempo_total // 60)\n",
    "    segundos = int(tempo_total % 60)\n",
    "    print(f\"‚è±Ô∏è Tempo total de execu√ß√£o: {minutos} minutos e {segundos} segundos\")\n",
    "    \n",
    "    return metadados_modelos\n",
    "\n",
    "def carregar_config_modelo(componente, passo):\n",
    "    \"\"\"Carrega a configura√ß√£o de um modelo espec√≠fico\"\"\"\n",
    "    try:\n",
    "        with open(f\"configsRF/config_{componente}_t{passo}.json\", 'r') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Configura√ß√£o n√£o encontrada para {componente}_t{passo}\")\n",
    "        return None\n",
    "\n",
    "# --------------------------------\n",
    "# C√ìDIGO PRINCIPAL PARA EXECU√á√ÉO\n",
    "# --------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Treinamento de RandomForest para componentes Wavelet de detalhe (D1, D2, D3)\")\n",
    "    \n",
    "    # Definir os caminhos dos arquivos CSV para cada componente de detalhe\n",
    "    caminhos_componentes = {\n",
    "        'd1': 'D1_component.csv',\n",
    "        'd2': 'D2_component.csv',\n",
    "        'd3': 'D3_component.csv'\n",
    "    }\n",
    "    \n",
    "    # Menu para sele√ß√£o de configura√ß√£o\n",
    "    print(\"\\nüìã Escolha uma configura√ß√£o:\")\n",
    "    print(\"1. Configura√ß√£o b√°sica (r√°pida)\")\n",
    "    print(\"2. Configura√ß√£o para detec√ß√£o de picos\")\n",
    "    print(\"3. Configura√ß√£o personalizada\")\n",
    "    \n",
    "    opcao = input(\"Op√ß√£o (1, 2 ou 3): \")\n",
    "    \n",
    "    # Definir par√¢metros com base na escolha\n",
    "    if opcao == \"1\":\n",
    "        # Configura√ß√£o b√°sica e r√°pida\n",
    "        rf_params = {\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 10,\n",
    "            'min_samples_split': 5,\n",
    "            'min_samples_leaf': 2,\n",
    "            'max_features': 'sqrt',\n",
    "            'bootstrap': True,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        look_back = 10\n",
    "        passos = [1]  # Apenas t+1 para teste r√°pido\n",
    "        \n",
    "    elif opcao == \"2\":\n",
    "        # Configura√ß√£o otimizada para detec√ß√£o de picos\n",
    "        rf_params = {\n",
    "            'n_estimators': 300,\n",
    "            'max_depth': 15,\n",
    "            'min_samples_split': 5,\n",
    "            'min_samples_leaf': 1,  # 1 para maior sensibilidade a outliers (picos)\n",
    "            'max_features': None,   # Usar todas as features\n",
    "            'bootstrap': True,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        look_back = 15\n",
    "        passos = [1, 7]  # t+1 e t+7 como amostra\n",
    "        \n",
    "    else:  # Op√ß√£o 3 ou qualquer outra entrada\n",
    "        # Configura√ß√£o personalizada\n",
    "        print(\"\\n‚öôÔ∏è Configure os par√¢metros do RandomForest:\")\n",
    "        \n",
    "        # N√∫mero de √°rvores\n",
    "        try:\n",
    "            n_estimators = int(input(\"N√∫mero de √°rvores (recomendado: 100-500) [200]: \") or \"200\")\n",
    "        except ValueError:\n",
    "            n_estimators = 200\n",
    "            print(\"Valor inv√°lido. Usando 200 √°rvores.\")\n",
    "        \n",
    "        # Profundidade m√°xima\n",
    "        try:\n",
    "            max_depth_input = input(\"Profundidade m√°xima (recomendado: 10-20, 'None' para ilimitado) [15]: \") or \"15\"\n",
    "            max_depth = None if max_depth_input.lower() == 'none' else int(max_depth_input)\n",
    "        except ValueError:\n",
    "            max_depth = 15\n",
    "            print(\"Valor inv√°lido. Usando profundidade 15.\")\n",
    "        \n",
    "        # min_samples_split\n",
    "        try:\n",
    "            min_samples_split = int(input(\"min_samples_split (recomendado: 2-10) [5]: \") or \"5\")\n",
    "        except ValueError:\n",
    "            min_samples_split = 5\n",
    "            print(\"Valor inv√°lido. Usando 5.\")\n",
    "        \n",
    "        # min_samples_leaf\n",
    "        try:\n",
    "            min_samples_leaf = int(input(\"min_samples_leaf (1 para mais sensibilidade a picos) [1]: \") or \"1\")\n",
    "        except ValueError:\n",
    "            min_samples_leaf = 1\n",
    "            print(\"Valor inv√°lido. Usando 1.\")\n",
    "        \n",
    "        # max_features\n",
    "        max_features_input = input(\"max_features ('sqrt', 'log2', 'None' para todas) [sqrt]: \") or \"sqrt\"\n",
    "        if max_features_input.lower() == 'none':\n",
    "            max_features = None\n",
    "        else:\n",
    "            max_features = max_features_input if max_features_input in ['sqrt', 'log2'] else 'sqrt'\n",
    "        \n",
    "        # Look-back\n",
    "        try:\n",
    "            look_back_input = input(\"look_back (recomendado: 10-20, 'None' para autom√°tico) [None]: \") or \"None\"\n",
    "            look_back = None if look_back_input.lower() == 'none' else int(look_back_input)\n",
    "        except ValueError:\n",
    "            look_back = None\n",
    "            print(\"Valor inv√°lido. Usando look_back autom√°tico.\")\n",
    "        \n",
    "        # Horizontes de previs√£o\n",
    "        passos_input = input(\"Horizontes de previs√£o (separados por v√≠rgula, ex: 1,7,30) [1]: \") or \"1\"\n",
    "        try:\n",
    "            passos = [int(p.strip()) for p in passos_input.split(\",\")]\n",
    "        except ValueError:\n",
    "            passos = [1]\n",
    "            print(\"Valor inv√°lido. Usando apenas horizonte t+1.\")\n",
    "        \n",
    "        # Montar par√¢metros\n",
    "        rf_params = {\n",
    "            'n_estimators': n_estimators,\n",
    "            'max_depth': max_depth,\n",
    "            'min_samples_split': min_samples_split,\n",
    "            'min_samples_leaf': min_samples_leaf,\n",
    "            'max_features': max_features,\n",
    "            'bootstrap': True,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "    \n",
    "    # Confirmar configura√ß√£o\n",
    "    print(\"\\n‚öôÔ∏è Configura√ß√£o selecionada:\")\n",
    "    print(f\"   - RandomForest: {rf_params}\")\n",
    "    print(f\"   - look_back: {'Autom√°tico' if look_back is None else look_back}\")\n",
    "    print(f\"   - Horizontes: {passos}\")\n",
    "    \n",
    "    confirma = input(\"\\nConfirma esta configura√ß√£o? (s/n): \").lower()\n",
    "    \n",
    "    if confirma == 's':\n",
    "        # Inicia o treinamento\n",
    "        print(\"\\nüèÉ Iniciando treinamento...\")\n",
    "        metadados = treinar_rf_para_componentes(caminhos_componentes, passos, rf_params, look_back)\n",
    "        \n",
    "        # Exibir resumo\n",
    "        print(\"\\n‚úÖ Treinamento conclu√≠do!\")\n",
    "        print(\"üìä Resumo dos modelos treinados:\")\n",
    "        \n",
    "        for comp, modelos in metadados.items():\n",
    "            print(f\"\\nüìå Componente: {comp.upper()}\")\n",
    "            for passo_key, info in modelos.items():\n",
    "                print(f\"   - Passo {info['passo']}: look_back={info['look_back']}, features={info['num_features']}\")\n",
    "                if 'oob_score' in info:\n",
    "                    print(f\"     OOB Score: {info['oob_score']:.4f}\")\n",
    "    else:\n",
    "        print(\"Treinamento cancelado.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
