{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6906a9e",
   "metadata": {},
   "source": [
    "## Salvado o SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7fba566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import joblib\n",
    "\n",
    "def treinar_svr_e_salvar(caminho_csv, coluna, look_back=10):\n",
    "    dados = pd.read_csv(caminho_csv)[coluna].values\n",
    "    scaler = MinMaxScaler()\n",
    "    dados_scaled = scaler.fit_transform(dados.reshape(-1, 1)).flatten()\n",
    "\n",
    "    X, y = [], []\n",
    "    for i in range(len(dados_scaled) - look_back):\n",
    "        X.append(dados_scaled[i:i+look_back])\n",
    "        y.append(dados_scaled[i+look_back])\n",
    "    X, y = np.array(X), np.array(y)\n",
    "\n",
    "    split = int(0.7 * len(X))\n",
    "    X_train, X_test, y_train, y_test = X[:split], X[split:], y[:split], y[split:]\n",
    "\n",
    "    model = SVR(C=10, epsilon=0.01, kernel='rbf')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Salvar modelo e scaler\n",
    "    joblib.dump(model, f\"svr_{coluna}.joblib\")\n",
    "    joblib.dump(scaler, f\"scaler_{coluna}.joblib\")\n",
    "\n",
    "    print(f\"[SVR] {coluna} modelo salvo como svr_{coluna}.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a33243e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SVR] D3 modelo salvo como svr_D3.joblib\n",
      "[SVR] D2 modelo salvo como svr_D2.joblib\n",
      "[SVR] D1 modelo salvo como svr_D1.joblib\n"
     ]
    }
   ],
   "source": [
    "treinar_svr_e_salvar(\"D3_component.csv\", \"D3\")\n",
    "treinar_svr_e_salvar(\"D2_component.csv\", \"D2\")\n",
    "treinar_svr_e_salvar(\"D1_component.csv\", \"D1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b07b3a7",
   "metadata": {},
   "source": [
    "## SVR para multiplos dias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65153c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import joblib\n",
    "\n",
    "def treinar_svr_multiplos_passos(caminho_csv, coluna, passos=[1, 3, 7, 30]):\n",
    "    dados = pd.read_csv(caminho_csv)[coluna].values\n",
    "    dados = dados.reshape(-1)  # j√° assumimos normalizado externamente\n",
    "\n",
    "    for passo in passos:\n",
    "        look_back = 5 if passo in [1, 3, 5] else 10\n",
    "\n",
    "        print(f\"‚è≥ Treinando SVR para {coluna} | t+{passo} (look_back={look_back})\")\n",
    "\n",
    "        X, y = criar_dataset_multi_step(dados, look_back=look_back, passo=passo)\n",
    "        split = int(0.7 * len(X))\n",
    "        X_train, X_test = X[:split], X[split:]\n",
    "        y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "        model = SVR(C=10, epsilon=0.01, kernel='rbf')\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        joblib.dump(model, f\"svr_{coluna}_t{passo}.joblib\")\n",
    "        print(f\"‚úÖ Modelo SVR salvo: svr_{coluna}_t{passo}.joblib\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36a1773",
   "metadata": {},
   "source": [
    "## SRV com Grid e Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c51f527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error\n",
    "import joblib\n",
    "\n",
    "def criar_dataset_multi_step(series, look_back=10, passo=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(series) - look_back - passo + 1):\n",
    "        X.append(series[i:i+look_back])\n",
    "        y.append(series[i+look_back+passo-1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def treinar_svr_multiplos_passos(caminho_csv, coluna, passos=[1, 3, 5, 7, 30]):\n",
    "    print(f\"\\nüìà GridSearchCV para SVR + RobustScaler no componente: {coluna}\")\n",
    "\n",
    "    # 1. Carregar os dados reais da componente (sem normaliza√ß√£o)\n",
    "    dados = pd.read_csv(caminho_csv)[coluna].values.reshape(-1)\n",
    "\n",
    "    for passo in passos:\n",
    "        look_back = 5 if passo in [1, 3, 5] else 10\n",
    "        print(f\"\\n‚è≥ Treinando SVR | {coluna} | t+{passo} | look_back={look_back}\")\n",
    "\n",
    "        X, y = criar_dataset_multi_step(dados, look_back=look_back, passo=passo)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # 2. Definir o pipeline\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', RobustScaler()),\n",
    "            ('svr', SVR())\n",
    "        ])\n",
    "\n",
    "        # 3. Grade de hiperpar√¢metros\n",
    "        param_grid = {\n",
    "            'svr__C': [1, 10, 50],\n",
    "            'svr__epsilon': [0.01, 0.1, 0.2],\n",
    "            'svr__gamma': ['scale', 'auto']\n",
    "        }\n",
    "\n",
    "        # 4. Configurar Grid Search com MAE\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=pipeline,\n",
    "            param_grid=param_grid,\n",
    "            scoring=make_scorer(mean_absolute_error, greater_is_better=False),\n",
    "            cv=5,\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # 5. Treinar com valida√ß√£o cruzada\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # 6. Melhor modelo encontrado\n",
    "        melhor_pipeline = grid_search.best_estimator_\n",
    "        print(f\"üîç Melhor conjunto de par√¢metros: {grid_search.best_params_}\")\n",
    "\n",
    "        # 7. Salvar o pipeline ajustado\n",
    "        joblib.dump(melhor_pipeline, f\"svr_{coluna}_t{passo}.joblib\")\n",
    "        print(f\"‚úÖ Pipeline final salvo: svr_{coluna}_t{passo}.joblib\")\n",
    "\n",
    "    print(\"\\n‚úÖ Todos os modelos treinados com GridSearch e salvos com sucesso.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b78b3cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà GridSearchCV para SVR + RobustScaler no componente: A3\n",
      "\n",
      "‚è≥ Treinando SVR | A3 | t+1 | look_back=5\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "üîç Melhor conjunto de par√¢metros: {'svr__C': 10, 'svr__epsilon': 0.2, 'svr__gamma': 'auto'}\n",
      "‚úÖ Pipeline final salvo: svr_A3_t1.joblib\n",
      "\n",
      "‚è≥ Treinando SVR | A3 | t+3 | look_back=5\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "üîç Melhor conjunto de par√¢metros: {'svr__C': 1, 'svr__epsilon': 0.2, 'svr__gamma': 'auto'}\n",
      "‚úÖ Pipeline final salvo: svr_A3_t3.joblib\n",
      "\n",
      "‚è≥ Treinando SVR | A3 | t+5 | look_back=5\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "üîç Melhor conjunto de par√¢metros: {'svr__C': 1, 'svr__epsilon': 0.01, 'svr__gamma': 'auto'}\n",
      "‚úÖ Pipeline final salvo: svr_A3_t5.joblib\n",
      "\n",
      "‚è≥ Treinando SVR | A3 | t+7 | look_back=10\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "üîç Melhor conjunto de par√¢metros: {'svr__C': 1, 'svr__epsilon': 0.1, 'svr__gamma': 'auto'}\n",
      "‚úÖ Pipeline final salvo: svr_A3_t7.joblib\n",
      "\n",
      "‚è≥ Treinando SVR | A3 | t+30 | look_back=10\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "üîç Melhor conjunto de par√¢metros: {'svr__C': 1, 'svr__epsilon': 0.2, 'svr__gamma': 'scale'}\n",
      "‚úÖ Pipeline final salvo: svr_A3_t30.joblib\n",
      "\n",
      "‚úÖ Todos os modelos treinados com GridSearch e salvos com sucesso.\n",
      "\n",
      "üìà GridSearchCV para SVR + RobustScaler no componente: D2\n",
      "\n",
      "‚è≥ Treinando SVR | D2 | t+1 | look_back=5\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "üîç Melhor conjunto de par√¢metros: {'svr__C': 1, 'svr__epsilon': 0.2, 'svr__gamma': 'auto'}\n",
      "‚úÖ Pipeline final salvo: svr_D2_t1.joblib\n",
      "\n",
      "‚è≥ Treinando SVR | D2 | t+3 | look_back=5\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "üîç Melhor conjunto de par√¢metros: {'svr__C': 1, 'svr__epsilon': 0.1, 'svr__gamma': 'auto'}\n",
      "‚úÖ Pipeline final salvo: svr_D2_t3.joblib\n",
      "\n",
      "‚è≥ Treinando SVR | D2 | t+5 | look_back=5\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "üîç Melhor conjunto de par√¢metros: {'svr__C': 1, 'svr__epsilon': 0.2, 'svr__gamma': 'auto'}\n",
      "‚úÖ Pipeline final salvo: svr_D2_t5.joblib\n",
      "\n",
      "‚è≥ Treinando SVR | D2 | t+7 | look_back=10\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "üîç Melhor conjunto de par√¢metros: {'svr__C': 1, 'svr__epsilon': 0.1, 'svr__gamma': 'auto'}\n",
      "‚úÖ Pipeline final salvo: svr_D2_t7.joblib\n",
      "\n",
      "‚è≥ Treinando SVR | D2 | t+30 | look_back=10\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "üîç Melhor conjunto de par√¢metros: {'svr__C': 1, 'svr__epsilon': 0.2, 'svr__gamma': 'auto'}\n",
      "‚úÖ Pipeline final salvo: svr_D2_t30.joblib\n",
      "\n",
      "‚úÖ Todos os modelos treinados com GridSearch e salvos com sucesso.\n",
      "\n",
      "üìà GridSearchCV para SVR + RobustScaler no componente: D1\n",
      "\n",
      "‚è≥ Treinando SVR | D1 | t+1 | look_back=5\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "üîç Melhor conjunto de par√¢metros: {'svr__C': 1, 'svr__epsilon': 0.1, 'svr__gamma': 'auto'}\n",
      "‚úÖ Pipeline final salvo: svr_D1_t1.joblib\n",
      "\n",
      "‚è≥ Treinando SVR | D1 | t+3 | look_back=5\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "üîç Melhor conjunto de par√¢metros: {'svr__C': 1, 'svr__epsilon': 0.01, 'svr__gamma': 'auto'}\n",
      "‚úÖ Pipeline final salvo: svr_D1_t3.joblib\n",
      "\n",
      "‚è≥ Treinando SVR | D1 | t+5 | look_back=5\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "üîç Melhor conjunto de par√¢metros: {'svr__C': 1, 'svr__epsilon': 0.1, 'svr__gamma': 'auto'}\n",
      "‚úÖ Pipeline final salvo: svr_D1_t5.joblib\n",
      "\n",
      "‚è≥ Treinando SVR | D1 | t+7 | look_back=10\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "üîç Melhor conjunto de par√¢metros: {'svr__C': 1, 'svr__epsilon': 0.1, 'svr__gamma': 'auto'}\n",
      "‚úÖ Pipeline final salvo: svr_D1_t7.joblib\n",
      "\n",
      "‚è≥ Treinando SVR | D1 | t+30 | look_back=10\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "üîç Melhor conjunto de par√¢metros: {'svr__C': 1, 'svr__epsilon': 0.1, 'svr__gamma': 'auto'}\n",
      "‚úÖ Pipeline final salvo: svr_D1_t30.joblib\n",
      "\n",
      "‚úÖ Todos os modelos treinados com GridSearch e salvos com sucesso.\n"
     ]
    }
   ],
   "source": [
    "treinar_svr_multiplos_passos(\"A3_component.csv\", \"A3\")\n",
    "treinar_svr_multiplos_passos(\"D2_component.csv\", \"D2\")\n",
    "treinar_svr_multiplos_passos(\"D1_component.csv\", \"D1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61722c5",
   "metadata": {},
   "source": [
    "## Abordagem alternativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b77ea4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "import datetime\n",
    "import logging\n",
    "from scipy import stats\n",
    "\n",
    "# Configura√ß√£o de logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"wavelet_svr_training.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "def criar_dataset_multi_step(series, look_back=10, passo=1, feature_engineering=True):\n",
    "    X, y = [], []\n",
    "    for i in range(len(series) - look_back - passo + 1):\n",
    "        features = series[i:i+look_back].copy()\n",
    "        if feature_engineering:\n",
    "            tendencia = features[-1] - features[0]\n",
    "            media_movel = np.mean(features)\n",
    "            volatilidade = np.std(features)\n",
    "            features = np.append(features, [tendencia, media_movel, volatilidade])\n",
    "        X.append(features)\n",
    "        y.append(series[i+look_back+passo-1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def avaliar_modelo(modelo, X_test, y_test, nome_modelo, coluna, passo):\n",
    "    y_pred = modelo.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / np.maximum(np.abs(y_test), 1e-10))) * 100\n",
    "\n",
    "    os.makedirs(\"resultados\", exist_ok=True)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')\n",
    "    plt.xlabel('Valores Reais')\n",
    "    plt.ylabel('Valores Previstos')\n",
    "    plt.title(f'Real vs Previsto - {coluna} (t+{passo})')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    n_show = min(100, len(y_test))\n",
    "    plt.plot(y_test[:n_show], label='Real', alpha=0.7)\n",
    "    plt.plot(y_pred[:n_show], label='Previsto', alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.title(f'Previs√£o vs Real - {coluna} (t+{passo})')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"resultados/{coluna}_t{passo}_resultados.png\")\n",
    "    plt.close()\n",
    "\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2, \"MAPE\": mape}\n",
    "\n",
    "def salvar_resultados_parciais(resultados, coluna):\n",
    "    os.makedirs(\"resultados\", exist_ok=True)\n",
    "    for passo, modelos in resultados.items():\n",
    "        resultados_passo_df = pd.DataFrame([\n",
    "            {\n",
    "                'Componente': coluna,\n",
    "                'Horizonte': passo,\n",
    "                'Modelo': modelo,\n",
    "                'Look_Back': info['look_back'],\n",
    "                'Feature_Engineering': info['feature_engineering'],\n",
    "                'MAE': info['metricas']['MAE'],\n",
    "                'RMSE': info['metricas']['RMSE'],\n",
    "                'R2': info['metricas']['R2'],\n",
    "                'MAPE': info['metricas']['MAPE']\n",
    "            }\n",
    "            for modelo, info in modelos.items()\n",
    "        ])\n",
    "        resultados_passo_df.to_csv(f\"resultados/{coluna}_t{passo}_resultados.csv\", index=False)\n",
    "\n",
    "def treinar_svr_multiplos_passos(caminho_csv, coluna, passos=[1, 3, 7, 30], testar_diferentes_scalers=True, feature_engineering=True):\n",
    "    logger.info(f\"Iniciando treinamento para componente: {coluna}\")\n",
    "    os.makedirs(\"modelos\", exist_ok=True)\n",
    "    try:\n",
    "        dados_df = pd.read_csv(caminho_csv)\n",
    "        dados = dados_df[coluna].values.reshape(-1)\n",
    "    except:\n",
    "        dados = dados_df.iloc[:, 0].values.reshape(-1)\n",
    "    dados = pd.Series(dados).interpolate().ffill().bfill().values\n",
    "    resultados = {passo: {} for passo in passos}\n",
    "    scalers = [('RobustScaler', RobustScaler()), ('StandardScaler', StandardScaler())] if testar_diferentes_scalers else [('RobustScaler', RobustScaler())]\n",
    "\n",
    "    for passo in passos:\n",
    "        if passo in [1, 3, 7]:\n",
    "            look_back_values = [5]\n",
    "        elif passo == 30:\n",
    "            look_back_values = [10]\n",
    "        else:\n",
    "            logger.warning(f\"Passo {passo} n√£o suportado explicitamente. Usando look_back=10 por padr√£o.\")\n",
    "            look_back_values = [10]\n",
    "\n",
    "        for look_back in look_back_values:\n",
    "            X, y = criar_dataset_multi_step(dados, look_back, passo, feature_engineering)\n",
    "            split = int(len(X) * 0.8)\n",
    "            X_train, X_test = X[:split], X[split:]\n",
    "            y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "            for scaler_nome, scaler in scalers:\n",
    "                pipeline = Pipeline([('scaler', scaler), ('svr', SVR())])\n",
    "                param_grid = {\n",
    "                    'svr__C': [1, 10],\n",
    "                    'svr__epsilon': [0.01, 0.1],\n",
    "                    'svr__gamma': ['scale', 0.1],\n",
    "                    'svr__kernel': ['rbf']\n",
    "                }\n",
    "                tscv = TimeSeriesSplit(n_splits=5)\n",
    "                grid = GridSearchCV(pipeline, param_grid, scoring='neg_mean_absolute_error', cv=tscv, n_jobs=-1, verbose=0)\n",
    "                try:\n",
    "                    grid.fit(X_train, y_train)\n",
    "                    best = grid.best_estimator_\n",
    "                    metricas = avaliar_modelo(best, X_test, y_test, f\"SVR_{scaler_nome}_lb{look_back}\", coluna, passo)\n",
    "                    resultados[passo][f\"SVR_{scaler_nome}_lb{look_back}\"] = {\n",
    "                        'metricas': metricas,\n",
    "                        'parametros': grid.best_params_,\n",
    "                        'look_back': look_back,\n",
    "                        'feature_engineering': feature_engineering\n",
    "                    }\n",
    "                    joblib.dump(best, f\"modelos/svr_{coluna}_t{passo}_{scaler_nome}_lb{look_back}.joblib\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Erro no GridSearchCV: {e}\")\n",
    "\n",
    "    salvar_resultados_parciais(resultados, coluna)\n",
    "    logger.info(f\"Modelos e resultados salvos para componente {coluna}\")\n",
    "    return resultados\n",
    "\n",
    "def treinar_componentes_wavelet(caminho_csv, componentes=['d1', 'd2', 'd3'], passos=[1, 3, 7, 30]):\n",
    "    for componente in componentes:\n",
    "        treinar_svr_multiplos_passos(caminho_csv, componente, passos)\n",
    "\n",
    "# Exemplo de execu√ß√£o:\n",
    "# treinar_componentes_wavelet(\"dados_wavelet.csv\", componentes=['d1'], passos=[1, 3, 5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cce02762",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 22:05:53,770 - INFO - Iniciando treinamento do componente D3...\n",
      "2025-05-17 22:05:53,771 - INFO - Iniciando treinamento para componente: D3\n",
      "2025-05-17 22:07:57,513 - INFO - Modelos e resultados salvos para componente D3\n",
      "2025-05-17 22:07:57,513 - INFO - Iniciando treinamento do componente D2...\n",
      "2025-05-17 22:07:57,514 - INFO - Iniciando treinamento para componente: D2\n",
      "2025-05-17 22:10:07,579 - INFO - Modelos e resultados salvos para componente D2\n",
      "2025-05-17 22:10:07,581 - INFO - Iniciando treinamento do componente D1...\n",
      "2025-05-17 22:10:07,583 - INFO - Iniciando treinamento para componente: D1\n",
      "2025-05-17 22:12:24,267 - INFO - Modelos e resultados salvos para componente D1\n"
     ]
    }
   ],
   "source": [
    "# Exemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    # Treinar modelos individualmente para cada componente conforme especificado\n",
    "    logger.info(\"Iniciando treinamento do componente D3...\")\n",
    "    treinar_svr_multiplos_passos(\"D3_component.csv\", \"D3\")\n",
    "    \n",
    "    logger.info(\"Iniciando treinamento do componente D2...\")\n",
    "    treinar_svr_multiplos_passos(\"D2_component.csv\", \"D2\")\n",
    "    \n",
    "    logger.info(\"Iniciando treinamento do componente D1...\")\n",
    "    treinar_svr_multiplos_passos(\"D1_component.csv\", \"D1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff697cd",
   "metadata": {},
   "source": [
    "## Salvando Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d15aa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "def criar_dataset_multi_step(series, look_back=10, passo=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(series) - look_back - passo + 1):\n",
    "        X.append(series[i:i + look_back])\n",
    "        y.append(series[i + look_back + passo - 1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def treinar_rf_para_componentes(caminhos_componentes, passos=[1, 5, 7, 30]):\n",
    "    os.makedirs(\"modelosRF\", exist_ok=True)\n",
    "    os.makedirs(\"scalersRF\", exist_ok=True)\n",
    "\n",
    "    for comp, caminho_csv in caminhos_componentes.items():\n",
    "        df = pd.read_csv(caminho_csv)\n",
    "        serie = df[comp.upper()].values.reshape(-1, 1)\n",
    "        serie = pd.Series(serie.flatten()).interpolate().ffill().bfill().values.reshape(-1, 1)\n",
    "\n",
    "        for passo in passos:\n",
    "            look_back = 5 if passo in [1, 5, 7] else 10\n",
    "\n",
    "            print(f\"üå≤ Treinando RandomForest para {comp.upper()} | t+{passo} (look_back={look_back})\")\n",
    "\n",
    "            # Normalizar\n",
    "            from sklearn.preprocessing import MinMaxScaler\n",
    "            scaler = MinMaxScaler()\n",
    "            serie_scaled = scaler.fit_transform(serie)\n",
    "\n",
    "            joblib.dump(scaler, f\"scalersRF/scaler_{comp}_t{passo}.joblib\")\n",
    "\n",
    "            # Criar dataset\n",
    "            X, y = criar_dataset_multi_step(serie_scaled, look_back=look_back, passo=passo)\n",
    "            split = int(0.8 * len(X))\n",
    "            X_train, y_train = X[:split], y[:split]\n",
    "            X_train = X_train.reshape((X_train.shape[0], X_train.shape[1]))\n",
    "\n",
    "            # Modelo mais robusto\n",
    "            model = RandomForestRegressor(\n",
    "                n_estimators=200,\n",
    "                max_depth=15,\n",
    "                min_samples_split=5,\n",
    "                min_samples_leaf=2,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            model.fit(X_train, y_train.ravel())\n",
    "\n",
    "            # Salvar modelo\n",
    "            joblib.dump(model, f\"modelosRF/rf_{comp}_t{passo}.joblib\")\n",
    "            print(f\"‚úÖ Modelo salvo: rf_{comp}_t{passo}.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d99a2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå≤ Treinando RandomForest para D1 | t+1 (look_back=5)\n",
      "‚úÖ Modelo salvo: rf_d1_t1.joblib\n",
      "üå≤ Treinando RandomForest para D1 | t+5 (look_back=5)\n",
      "‚úÖ Modelo salvo: rf_d1_t5.joblib\n",
      "üå≤ Treinando RandomForest para D1 | t+7 (look_back=5)\n",
      "‚úÖ Modelo salvo: rf_d1_t7.joblib\n",
      "üå≤ Treinando RandomForest para D1 | t+30 (look_back=10)\n",
      "‚úÖ Modelo salvo: rf_d1_t30.joblib\n",
      "üå≤ Treinando RandomForest para D2 | t+1 (look_back=5)\n",
      "‚úÖ Modelo salvo: rf_d2_t1.joblib\n",
      "üå≤ Treinando RandomForest para D2 | t+5 (look_back=5)\n",
      "‚úÖ Modelo salvo: rf_d2_t5.joblib\n",
      "üå≤ Treinando RandomForest para D2 | t+7 (look_back=5)\n",
      "‚úÖ Modelo salvo: rf_d2_t7.joblib\n",
      "üå≤ Treinando RandomForest para D2 | t+30 (look_back=10)\n",
      "‚úÖ Modelo salvo: rf_d2_t30.joblib\n",
      "üå≤ Treinando RandomForest para D3 | t+1 (look_back=5)\n",
      "‚úÖ Modelo salvo: rf_d3_t1.joblib\n",
      "üå≤ Treinando RandomForest para D3 | t+5 (look_back=5)\n",
      "‚úÖ Modelo salvo: rf_d3_t5.joblib\n",
      "üå≤ Treinando RandomForest para D3 | t+7 (look_back=5)\n",
      "‚úÖ Modelo salvo: rf_d3_t7.joblib\n",
      "üå≤ Treinando RandomForest para D3 | t+30 (look_back=10)\n",
      "‚úÖ Modelo salvo: rf_d3_t30.joblib\n"
     ]
    }
   ],
   "source": [
    "treinar_rf_para_componentes({\n",
    "    'd1': 'D1_component.csv',\n",
    "    'd2': 'D2_component.csv',\n",
    "    'd3': 'D3_component.csv'\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27309e8c",
   "metadata": {},
   "source": [
    "## Random forest com mais contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37d0488d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Treinamento de RandomForest para componentes Wavelet de detalhe (D1, D2, D3)\n",
      "\n",
      "üìã Escolha uma configura√ß√£o:\n",
      "1. Configura√ß√£o b√°sica (r√°pida)\n",
      "2. Configura√ß√£o para detec√ß√£o de picos\n",
      "3. Configura√ß√£o personalizada\n",
      "\n",
      "‚öôÔ∏è Configura√ß√£o selecionada:\n",
      "   - RandomForest: {'n_estimators': 300, 'max_depth': 15, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None, 'bootstrap': True, 'random_state': 42, 'n_jobs': -1}\n",
      "   - look_back: 15\n",
      "   - Horizontes: [1, 7]\n",
      "\n",
      "üèÉ Iniciando treinamento...\n",
      "\n",
      "‚öôÔ∏è Configura√ß√£o de Treinamento:\n",
      "   - Componentes: ['d1', 'd2', 'd3']\n",
      "   - Horizontes de previs√£o: [1, 7]\n",
      "   - Par√¢metros RF:\n",
      "      - n_estimators: 300\n",
      "      - max_depth: 15\n",
      "      - min_samples_split: 5\n",
      "      - min_samples_leaf: 1\n",
      "      - max_features: None\n",
      "      - bootstrap: True\n",
      "      - random_state: 42\n",
      "      - n_jobs: -1\n",
      "\n",
      "üîç Processando componente: D1\n",
      "   - Tamanho do dataset: 7878 pontos\n",
      "\n",
      "üå≤ Treinando RandomForest para D1 | t+1 (look_back=15)\n",
      "   - Dataset: 7863 amostras, 24 features\n",
      "   - Iniciando treinamento...\n",
      "   - Import√¢ncia das features:\n",
      "      - Features originais: 0.5275 (52.8%)\n",
      "      - Features engineered: 0.4725 (47.2%)\n",
      "      - Top 5 features:\n",
      "         1. m√©dia: 0.3547\n",
      "         2. t-15: 0.1669\n",
      "         3. t-14: 0.1046\n",
      "         4. t-1: 0.0693\n",
      "         5. t-13: 0.0451\n",
      "‚úÖ Modelo salvo: modelosRF/rf_d1_t1.joblib\n",
      "üìù Configura√ß√£o salva: configsRF/config_d1_t1.json\n",
      "‚è±Ô∏è Tempo de treinamento para t+1: 9.41 segundos\n",
      "\n",
      "üå≤ Treinando RandomForest para D1 | t+7 (look_back=15)\n",
      "   - Dataset: 7857 amostras, 24 features\n",
      "   - Iniciando treinamento...\n",
      "   - Import√¢ncia das features:\n",
      "      - Features originais: 0.7076 (70.8%)\n",
      "      - Features engineered: 0.2924 (29.2%)\n",
      "      - Top 5 features:\n",
      "         1. m√©dia: 0.0558\n",
      "         2. t-7: 0.0557\n",
      "         3. t-6: 0.0520\n",
      "         4. t-5: 0.0515\n",
      "         5. t-4: 0.0495\n",
      "‚úÖ Modelo salvo: modelosRF/rf_d1_t7.joblib\n",
      "üìù Configura√ß√£o salva: configsRF/config_d1_t7.json\n",
      "‚è±Ô∏è Tempo de treinamento para t+7: 13.24 segundos\n",
      "‚è±Ô∏è Tempo total para componente D1: 22.68 segundos\n",
      "\n",
      "üîç Processando componente: D2\n",
      "   - Tamanho do dataset: 7878 pontos\n",
      "\n",
      "üå≤ Treinando RandomForest para D2 | t+1 (look_back=15)\n",
      "   - Dataset: 7863 amostras, 24 features\n",
      "   - Iniciando treinamento...\n",
      "   - Import√¢ncia das features:\n",
      "      - Features originais: 0.7939 (79.4%)\n",
      "      - Features engineered: 0.2061 (20.6%)\n",
      "      - Top 5 features:\n",
      "         1. t-14: 0.3156\n",
      "         2. t-15: 0.2274\n",
      "         3. 2¬™_derivada_m√©dia: 0.1513\n",
      "         4. t-13: 0.1144\n",
      "         5. t-12: 0.0404\n",
      "‚úÖ Modelo salvo: modelosRF/rf_d2_t1.joblib\n",
      "üìù Configura√ß√£o salva: configsRF/config_d2_t1.json\n",
      "‚è±Ô∏è Tempo de treinamento para t+1: 11.93 segundos\n",
      "\n",
      "üå≤ Treinando RandomForest para D2 | t+7 (look_back=15)\n",
      "   - Dataset: 7857 amostras, 24 features\n",
      "   - Iniciando treinamento...\n",
      "   - Import√¢ncia das features:\n",
      "      - Features originais: 0.6999 (70.0%)\n",
      "      - Features engineered: 0.3001 (30.0%)\n",
      "      - Top 5 features:\n",
      "         1. t-15: 0.0847\n",
      "         2. m√©dia: 0.0588\n",
      "         3. t-13: 0.0572\n",
      "         4. t-14: 0.0550\n",
      "         5. 2¬™_derivada_m√©dia: 0.0525\n",
      "‚úÖ Modelo salvo: modelosRF/rf_d2_t7.joblib\n",
      "üìù Configura√ß√£o salva: configsRF/config_d2_t7.json\n",
      "‚è±Ô∏è Tempo de treinamento para t+7: 14.23 segundos\n",
      "‚è±Ô∏è Tempo total para componente D2: 26.19 segundos\n",
      "\n",
      "üîç Processando componente: D3\n",
      "   - Tamanho do dataset: 7878 pontos\n",
      "\n",
      "üå≤ Treinando RandomForest para D3 | t+1 (look_back=15)\n",
      "   - Dataset: 7863 amostras, 24 features\n",
      "   - Iniciando treinamento...\n",
      "   - Import√¢ncia das features:\n",
      "      - Features originais: 0.9548 (95.5%)\n",
      "      - Features engineered: 0.0452 (4.5%)\n",
      "      - Top 5 features:\n",
      "         1. t-15: 0.6648\n",
      "         2. t-12: 0.1076\n",
      "         3. t-13: 0.0953\n",
      "         4. t-14: 0.0416\n",
      "         5. 2¬™_derivada_m√©dia: 0.0268\n",
      "‚úÖ Modelo salvo: modelosRF/rf_d3_t1.joblib\n",
      "üìù Configura√ß√£o salva: configsRF/config_d3_t1.json\n",
      "‚è±Ô∏è Tempo de treinamento para t+1: 13.99 segundos\n",
      "\n",
      "üå≤ Treinando RandomForest para D3 | t+7 (look_back=15)\n",
      "   - Dataset: 7857 amostras, 24 features\n",
      "   - Iniciando treinamento...\n",
      "   - Import√¢ncia das features:\n",
      "      - Features originais: 0.6951 (69.5%)\n",
      "      - Features engineered: 0.3049 (30.5%)\n",
      "      - Top 5 features:\n",
      "         1. t-15: 0.1590\n",
      "         2. 2¬™_derivada_m√©dia: 0.1119\n",
      "         3. t-14: 0.0663\n",
      "         4. m√©dia: 0.0644\n",
      "         5. t-13: 0.0492\n",
      "‚úÖ Modelo salvo: modelosRF/rf_d3_t7.joblib\n",
      "üìù Configura√ß√£o salva: configsRF/config_d3_t7.json\n",
      "‚è±Ô∏è Tempo de treinamento para t+7: 14.41 segundos\n",
      "‚è±Ô∏è Tempo total para componente D3: 28.45 segundos\n",
      "\n",
      "üìä Metadados de todos os modelos salvos em: configsRF/metadados_global.json\n",
      "‚è±Ô∏è Tempo total de execu√ß√£o: 1 minutos e 17 segundos\n",
      "\n",
      "‚úÖ Treinamento conclu√≠do!\n",
      "üìä Resumo dos modelos treinados:\n",
      "\n",
      "üìå Componente: D1\n",
      "   - Passo 1: look_back=15, features=24\n",
      "   - Passo 7: look_back=15, features=24\n",
      "\n",
      "üìå Componente: D2\n",
      "   - Passo 1: look_back=15, features=24\n",
      "   - Passo 7: look_back=15, features=24\n",
      "\n",
      "üìå Componente: D3\n",
      "   - Passo 1: look_back=15, features=24\n",
      "   - Passo 7: look_back=15, features=24\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import json\n",
    "import time\n",
    "\n",
    "def criar_dataset_multi_step(series, look_back=10, passo=1, adicionar_features=True):\n",
    "    X, y = [], []\n",
    "    for i in range(len(series) - look_back - passo + 1):\n",
    "        # Extrair a sequ√™ncia base\n",
    "        sequencia = series[i:i + look_back].flatten()\n",
    "        \n",
    "        if adicionar_features:\n",
    "            # Adicionar caracter√≠sticas para detec√ß√£o de picos\n",
    "            media_movel = np.mean(sequencia)\n",
    "            std_movel = np.std(sequencia)\n",
    "            max_local = np.max(sequencia)\n",
    "            min_local = np.min(sequencia)\n",
    "            amplitude = max_local - min_local\n",
    "            tendencia = sequencia[-1] - sequencia[0]\n",
    "            \n",
    "            # Derivada (taxa de mudan√ßa)\n",
    "            derivada = np.diff(sequencia)\n",
    "            derivada_mean = np.mean(derivada) if len(derivada) > 0 else 0\n",
    "            derivada_std = np.std(derivada) if len(derivada) > 0 else 0\n",
    "            \n",
    "            # Segunda derivada (acelera√ß√£o)\n",
    "            segunda_derivada = np.diff(derivada) if len(derivada) > 1 else np.array([0])\n",
    "            segunda_derivada_mean = np.mean(segunda_derivada) if len(segunda_derivada) > 0 else 0\n",
    "            \n",
    "            # Features adicionais\n",
    "            features = np.array([media_movel, std_movel, max_local, min_local, \n",
    "                               amplitude, tendencia, derivada_mean, derivada_std, \n",
    "                               segunda_derivada_mean])\n",
    "            \n",
    "            # Combinar features originais com as novas features\n",
    "            X.append(np.concatenate([sequencia, features]))\n",
    "        else:\n",
    "            X.append(sequencia)\n",
    "            \n",
    "        y.append(series[i + look_back + passo - 1])\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def treinar_rf_para_componentes(caminhos_componentes, passos=[1], rf_params=None, look_back=None):\n",
    "    \"\"\"\n",
    "    Treina modelos RandomForest para componentes wavelet.\n",
    "    \n",
    "    Args:\n",
    "        caminhos_componentes: Dicion√°rio com nome do componente e caminho do arquivo CSV\n",
    "        passos: Lista de horizontes de previs√£o (t+n)\n",
    "        rf_params: Dicion√°rio com par√¢metros para o RandomForestRegressor\n",
    "        look_back: Valor de look_back para a janela de hist√≥rico. Se None, ser√° calculado com base no passo.\n",
    "    \n",
    "    Returns:\n",
    "        Dicion√°rio com metadados dos modelos treinados\n",
    "    \"\"\"\n",
    "    # Tempo de in√≠cio\n",
    "    tempo_inicio = time.time()\n",
    "    \n",
    "    # Par√¢metros padr√£o do RandomForest se n√£o forem especificados\n",
    "    if rf_params is None:\n",
    "        rf_params = {\n",
    "            'n_estimators': 200,\n",
    "            'max_depth': 15,\n",
    "            'min_samples_split': 5,\n",
    "            'min_samples_leaf': 1,\n",
    "            'max_features': 'sqrt',\n",
    "            'bootstrap': True,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "    \n",
    "    # Criar diret√≥rios necess√°rios\n",
    "    os.makedirs(\"modelosRF\", exist_ok=True)\n",
    "    os.makedirs(\"scalersRF\", exist_ok=True)\n",
    "    os.makedirs(\"configsRF\", exist_ok=True)\n",
    "    \n",
    "    # Dicion√°rio para armazenar metadados de todos os modelos\n",
    "    metadados_modelos = {}\n",
    "    \n",
    "    # Exibir configura√ß√£o\n",
    "    print(\"\\n‚öôÔ∏è Configura√ß√£o de Treinamento:\")\n",
    "    print(f\"   - Componentes: {list(caminhos_componentes.keys())}\")\n",
    "    print(f\"   - Horizontes de previs√£o: {passos}\")\n",
    "    print(f\"   - Par√¢metros RF:\")\n",
    "    for param, valor in rf_params.items():\n",
    "        print(f\"      - {param}: {valor}\")\n",
    "    \n",
    "    # Percorrer cada componente\n",
    "    for comp, caminho_csv in caminhos_componentes.items():\n",
    "        tempo_componente = time.time()\n",
    "        print(f\"\\nüîç Processando componente: {comp.upper()}\")\n",
    "        \n",
    "        try:\n",
    "            # Carregar dados\n",
    "            df = pd.read_csv(caminho_csv)\n",
    "            \n",
    "            # Garantir que o componente exista no DataFrame\n",
    "            coluna_componente = comp.upper()\n",
    "            if coluna_componente not in df.columns:\n",
    "                print(f\"‚ö†Ô∏è Coluna {coluna_componente} n√£o encontrada em {caminho_csv}\")\n",
    "                continue\n",
    "                \n",
    "            serie = df[coluna_componente].values.reshape(-1, 1)\n",
    "            \n",
    "            # Limpeza e interpola√ß√£o de dados faltantes\n",
    "            serie = pd.Series(serie.flatten()).interpolate().ffill().bfill().values.reshape(-1, 1)\n",
    "            \n",
    "            # Verificar se h√° valores infinitos ou NaN\n",
    "            if np.isnan(serie).any() or np.isinf(serie).any():\n",
    "                print(f\"‚ö†Ô∏è Valores NaN ou infinitos encontrados em {comp}. Aplicando limpeza...\")\n",
    "                serie = np.nan_to_num(serie, nan=np.nanmean(serie), posinf=np.nanmax(serie), neginf=np.nanmin(serie))\n",
    "            \n",
    "            # Tamanho do dataset\n",
    "            print(f\"   - Tamanho do dataset: {len(serie)} pontos\")\n",
    "            \n",
    "            # Metadados espec√≠ficos do componente\n",
    "            metadados_modelos[comp] = {}\n",
    "            \n",
    "            # Treinar para cada horizonte de previs√£o\n",
    "            for passo in passos:\n",
    "                tempo_passo = time.time()\n",
    "                \n",
    "                # Calcular look_back se n√£o foi especificado\n",
    "                lookback_atual = look_back if look_back is not None else max(10, passo // 2)\n",
    "                \n",
    "                print(f\"\\nüå≤ Treinando RandomForest para {comp.upper()} | t+{passo} (look_back={lookback_atual})\")\n",
    "                \n",
    "                # Usar RobustScaler para lidar melhor com outliers\n",
    "                scaler = RobustScaler()\n",
    "                serie_scaled = scaler.fit_transform(serie)\n",
    "                \n",
    "                # Salvar scaler\n",
    "                scaler_filename = f\"scalersRF/scaler_{comp}_t{passo}.joblib\"\n",
    "                joblib.dump(scaler, scaler_filename)\n",
    "                \n",
    "                # Criar dataset com features adicionais\n",
    "                X, y = criar_dataset_multi_step(serie_scaled, look_back=lookback_atual, passo=passo, adicionar_features=True)\n",
    "                \n",
    "                # Informa√ß√µes sobre o dataset\n",
    "                print(f\"   - Dataset: {X.shape[0]} amostras, {X.shape[1]} features\")\n",
    "                \n",
    "                # Divis√£o treino/teste\n",
    "                split = int(0.8 * len(X))\n",
    "                X_train = X[:split]\n",
    "                y_train = y[:split]\n",
    "                \n",
    "                # Dicion√°rio para armazenar metadados do modelo\n",
    "                modelo_info = {\n",
    "                    'componente': comp,\n",
    "                    'passo': passo,\n",
    "                    'look_back': lookback_atual,\n",
    "                    'num_features': X.shape[1],\n",
    "                    'num_amostras_treino': len(X_train),\n",
    "                    'data_treinamento': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'usa_features_adicionais': True,\n",
    "                    'parametros': rf_params\n",
    "                }\n",
    "                \n",
    "                # Criar e treinar modelo com os par√¢metros fornecidos\n",
    "                print(f\"   - Iniciando treinamento...\")\n",
    "                model = RandomForestRegressor(**rf_params)\n",
    "                model.fit(X_train, y_train.ravel())\n",
    "                \n",
    "                # Se OOB score estiver dispon√≠vel, salvar\n",
    "                if hasattr(model, 'oob_score_'):\n",
    "                    modelo_info['oob_score'] = float(model.oob_score_)\n",
    "                    print(f\"   - OOB Score: {model.oob_score_:.4f}\")\n",
    "                \n",
    "                # Calcular e mostrar import√¢ncia das features\n",
    "                if len(model.feature_importances_) == X.shape[1]:\n",
    "                    # N√∫mero base de features originais\n",
    "                    num_features_orig = lookback_atual\n",
    "                    \n",
    "                    # Import√¢ncia das features originais (valores passados)\n",
    "                    imp_features_orig = model.feature_importances_[:num_features_orig].sum()\n",
    "                    \n",
    "                    # Import√¢ncia das features engineered\n",
    "                    imp_features_eng = model.feature_importances_[num_features_orig:].sum()\n",
    "                    \n",
    "                    print(f\"   - Import√¢ncia das features:\")\n",
    "                    print(f\"      - Features originais: {imp_features_orig:.4f} ({imp_features_orig*100:.1f}%)\")\n",
    "                    print(f\"      - Features engineered: {imp_features_eng:.4f} ({imp_features_eng*100:.1f}%)\")\n",
    "                    \n",
    "                    # Top 5 features mais importantes\n",
    "                    top_indices = np.argsort(model.feature_importances_)[-5:][::-1]\n",
    "                    \n",
    "                    # Nomes das features\n",
    "                    feature_names = [f\"t-{i+1}\" for i in range(lookback_atual)]\n",
    "                    feature_names.extend([\"m√©dia\", \"std\", \"max\", \"min\", \"amplitude\", \n",
    "                                          \"tend√™ncia\", \"derivada_m√©dia\", \"derivada_std\", \n",
    "                                          \"2¬™_derivada_m√©dia\"])\n",
    "                    \n",
    "                    print(f\"      - Top 5 features:\")\n",
    "                    for i, idx in enumerate(top_indices):\n",
    "                        if idx < len(feature_names):\n",
    "                            print(f\"         {i+1}. {feature_names[idx]}: {model.feature_importances_[idx]:.4f}\")\n",
    "                        else:\n",
    "                            print(f\"         {i+1}. Feature {idx}: {model.feature_importances_[idx]:.4f}\")\n",
    "                    \n",
    "                    # Salvar import√¢ncia das features no modelo_info\n",
    "                    modelo_info['importancia_features'] = {\n",
    "                        'features_originais': float(imp_features_orig),\n",
    "                        'features_engineered': float(imp_features_eng),\n",
    "                        'top_5_indices': top_indices.tolist(),\n",
    "                        'top_5_valores': model.feature_importances_[top_indices].tolist(),\n",
    "                        'todas_importancias': model.feature_importances_.tolist()\n",
    "                    }\n",
    "                \n",
    "                # Salvar modelo\n",
    "                modelo_filename = f\"modelosRF/rf_{comp}_t{passo}.joblib\"\n",
    "                joblib.dump(model, modelo_filename)\n",
    "                \n",
    "                # Salvar metadados do modelo\n",
    "                config_filename = f\"configsRF/config_{comp}_t{passo}.json\"\n",
    "                with open(config_filename, 'w') as f:\n",
    "                    json.dump(modelo_info, f, indent=4)\n",
    "                \n",
    "                # Adicionar informa√ß√£o ao dicion√°rio global\n",
    "                metadados_modelos[comp][f't{passo}'] = modelo_info\n",
    "                \n",
    "                # Tempo de treinamento para este horizonte\n",
    "                tempo_passo_fim = time.time() - tempo_passo\n",
    "                print(f\"‚úÖ Modelo salvo: {modelo_filename}\")\n",
    "                print(f\"üìù Configura√ß√£o salva: {config_filename}\")\n",
    "                print(f\"‚è±Ô∏è Tempo de treinamento para t+{passo}: {tempo_passo_fim:.2f} segundos\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao processar componente {comp}: {str(e)}\")\n",
    "        \n",
    "        # Tempo para este componente\n",
    "        tempo_componente_fim = time.time() - tempo_componente\n",
    "        print(f\"‚è±Ô∏è Tempo total para componente {comp.upper()}: {tempo_componente_fim:.2f} segundos\")\n",
    "    \n",
    "    # Salvar metadados globais\n",
    "    with open(\"configsRF/metadados_global.json\", 'w') as f:\n",
    "        json.dump(metadados_modelos, f, indent=4)\n",
    "    \n",
    "    print(f\"\\nüìä Metadados de todos os modelos salvos em: configsRF/metadados_global.json\")\n",
    "    \n",
    "    # Tempo total\n",
    "    tempo_total = time.time() - tempo_inicio\n",
    "    minutos = int(tempo_total // 60)\n",
    "    segundos = int(tempo_total % 60)\n",
    "    print(f\"‚è±Ô∏è Tempo total de execu√ß√£o: {minutos} minutos e {segundos} segundos\")\n",
    "    \n",
    "    return metadados_modelos\n",
    "\n",
    "def carregar_config_modelo(componente, passo):\n",
    "    \"\"\"Carrega a configura√ß√£o de um modelo espec√≠fico\"\"\"\n",
    "    try:\n",
    "        with open(f\"configsRF/config_{componente}_t{passo}.json\", 'r') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Configura√ß√£o n√£o encontrada para {componente}_t{passo}\")\n",
    "        return None\n",
    "\n",
    "# --------------------------------\n",
    "# C√ìDIGO PRINCIPAL PARA EXECU√á√ÉO\n",
    "# --------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Treinamento de RandomForest para componentes Wavelet de detalhe (D1, D2, D3)\")\n",
    "    \n",
    "    # Definir os caminhos dos arquivos CSV para cada componente de detalhe\n",
    "    caminhos_componentes = {\n",
    "        'd1': 'D1_component.csv',\n",
    "        'd2': 'D2_component.csv',\n",
    "        'd3': 'D3_component.csv'\n",
    "    }\n",
    "    \n",
    "    # Menu para sele√ß√£o de configura√ß√£o\n",
    "    print(\"\\nüìã Escolha uma configura√ß√£o:\")\n",
    "    print(\"1. Configura√ß√£o b√°sica (r√°pida)\")\n",
    "    print(\"2. Configura√ß√£o para detec√ß√£o de picos\")\n",
    "    print(\"3. Configura√ß√£o personalizada\")\n",
    "    \n",
    "    opcao = input(\"Op√ß√£o (1, 2 ou 3): \")\n",
    "    \n",
    "    # Definir par√¢metros com base na escolha\n",
    "    if opcao == \"1\":\n",
    "        # Configura√ß√£o b√°sica e r√°pida\n",
    "        rf_params = {\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 10,\n",
    "            'min_samples_split': 5,\n",
    "            'min_samples_leaf': 2,\n",
    "            'max_features': 'sqrt',\n",
    "            'bootstrap': True,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        look_back = 10\n",
    "        passos = [1]  # Apenas t+1 para teste r√°pido\n",
    "        \n",
    "    elif opcao == \"2\":\n",
    "        # Configura√ß√£o otimizada para detec√ß√£o de picos\n",
    "        rf_params = {\n",
    "            'n_estimators': 300,\n",
    "            'max_depth': 15,\n",
    "            'min_samples_split': 5,\n",
    "            'min_samples_leaf': 1,  # 1 para maior sensibilidade a outliers (picos)\n",
    "            'max_features': None,   # Usar todas as features\n",
    "            'bootstrap': True,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        look_back = 15\n",
    "        passos = [1, 7]  # t+1 e t+7 como amostra\n",
    "        \n",
    "    else:  # Op√ß√£o 3 ou qualquer outra entrada\n",
    "        # Configura√ß√£o personalizada\n",
    "        print(\"\\n‚öôÔ∏è Configure os par√¢metros do RandomForest:\")\n",
    "        \n",
    "        # N√∫mero de √°rvores\n",
    "        try:\n",
    "            n_estimators = int(input(\"N√∫mero de √°rvores (recomendado: 100-500) [200]: \") or \"200\")\n",
    "        except ValueError:\n",
    "            n_estimators = 200\n",
    "            print(\"Valor inv√°lido. Usando 200 √°rvores.\")\n",
    "        \n",
    "        # Profundidade m√°xima\n",
    "        try:\n",
    "            max_depth_input = input(\"Profundidade m√°xima (recomendado: 10-20, 'None' para ilimitado) [15]: \") or \"15\"\n",
    "            max_depth = None if max_depth_input.lower() == 'none' else int(max_depth_input)\n",
    "        except ValueError:\n",
    "            max_depth = 15\n",
    "            print(\"Valor inv√°lido. Usando profundidade 15.\")\n",
    "        \n",
    "        # min_samples_split\n",
    "        try:\n",
    "            min_samples_split = int(input(\"min_samples_split (recomendado: 2-10) [5]: \") or \"5\")\n",
    "        except ValueError:\n",
    "            min_samples_split = 5\n",
    "            print(\"Valor inv√°lido. Usando 5.\")\n",
    "        \n",
    "        # min_samples_leaf\n",
    "        try:\n",
    "            min_samples_leaf = int(input(\"min_samples_leaf (1 para mais sensibilidade a picos) [1]: \") or \"1\")\n",
    "        except ValueError:\n",
    "            min_samples_leaf = 1\n",
    "            print(\"Valor inv√°lido. Usando 1.\")\n",
    "        \n",
    "        # max_features\n",
    "        max_features_input = input(\"max_features ('sqrt', 'log2', 'None' para todas) [sqrt]: \") or \"sqrt\"\n",
    "        if max_features_input.lower() == 'none':\n",
    "            max_features = None\n",
    "        else:\n",
    "            max_features = max_features_input if max_features_input in ['sqrt', 'log2'] else 'sqrt'\n",
    "        \n",
    "        # Look-back\n",
    "        try:\n",
    "            look_back_input = input(\"look_back (recomendado: 10-20, 'None' para autom√°tico) [None]: \") or \"None\"\n",
    "            look_back = None if look_back_input.lower() == 'none' else int(look_back_input)\n",
    "        except ValueError:\n",
    "            look_back = None\n",
    "            print(\"Valor inv√°lido. Usando look_back autom√°tico.\")\n",
    "        \n",
    "        # Horizontes de previs√£o\n",
    "        passos_input = input(\"Horizontes de previs√£o (separados por v√≠rgula, ex: 1,7,30) [1]: \") or \"1\"\n",
    "        try:\n",
    "            passos = [int(p.strip()) for p in passos_input.split(\",\")]\n",
    "        except ValueError:\n",
    "            passos = [1]\n",
    "            print(\"Valor inv√°lido. Usando apenas horizonte t+1.\")\n",
    "        \n",
    "        # Montar par√¢metros\n",
    "        rf_params = {\n",
    "            'n_estimators': n_estimators,\n",
    "            'max_depth': max_depth,\n",
    "            'min_samples_split': min_samples_split,\n",
    "            'min_samples_leaf': min_samples_leaf,\n",
    "            'max_features': max_features,\n",
    "            'bootstrap': True,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "    \n",
    "    # Confirmar configura√ß√£o\n",
    "    print(\"\\n‚öôÔ∏è Configura√ß√£o selecionada:\")\n",
    "    print(f\"   - RandomForest: {rf_params}\")\n",
    "    print(f\"   - look_back: {'Autom√°tico' if look_back is None else look_back}\")\n",
    "    print(f\"   - Horizontes: {passos}\")\n",
    "    \n",
    "    confirma = input(\"\\nConfirma esta configura√ß√£o? (s/n): \").lower()\n",
    "    \n",
    "    if confirma == 's':\n",
    "        # Inicia o treinamento\n",
    "        print(\"\\nüèÉ Iniciando treinamento...\")\n",
    "        metadados = treinar_rf_para_componentes(caminhos_componentes, passos, rf_params, look_back)\n",
    "        \n",
    "        # Exibir resumo\n",
    "        print(\"\\n‚úÖ Treinamento conclu√≠do!\")\n",
    "        print(\"üìä Resumo dos modelos treinados:\")\n",
    "        \n",
    "        for comp, modelos in metadados.items():\n",
    "            print(f\"\\nüìå Componente: {comp.upper()}\")\n",
    "            for passo_key, info in modelos.items():\n",
    "                print(f\"   - Passo {info['passo']}: look_back={info['look_back']}, features={info['num_features']}\")\n",
    "                if 'oob_score' in info:\n",
    "                    print(f\"     OOB Score: {info['oob_score']:.4f}\")\n",
    "    else:\n",
    "        print(\"Treinamento cancelado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905f2bb7",
   "metadata": {},
   "source": [
    "### Salvado oXGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b029f440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Fun√ß√£o para criar dataset multi-step\n",
    "def criar_dataset_multi_step(series, look_back=10, passo=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(series) - look_back - passo + 1):\n",
    "        X.append(series[i:i+look_back])\n",
    "        y.append(series[i+look_back+passo-1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Fun√ß√£o para treinar XGBoost para m√∫ltiplos passos\n",
    "def treinar_xgb_multiplos_passos(caminho_csv, coluna, passos=[1, 3, 5, 7, 30]):\n",
    "    dados = pd.read_csv(caminho_csv)[coluna].values\n",
    "    dados = dados.reshape(-1)  # j√° normalizado externamente\n",
    "\n",
    "    for passo in passos:\n",
    "        look_back = 5 if passo in [1, 3, 5] else 10\n",
    "\n",
    "        print(f\"üöÄ Treinando XGBoost para {coluna} | t+{passo} (look_back={look_back})\")\n",
    "\n",
    "        X, y = criar_dataset_multi_step(dados, look_back=look_back, passo=passo)\n",
    "\n",
    "        split = int(0.7 * len(X))\n",
    "        X_train, X_test = X[:split], X[split:]\n",
    "        y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "        model = XGBRegressor()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Salvar modelo\n",
    "        joblib.dump(model, f\"xgb_{coluna}_t{passo}.joblib\")\n",
    "        print(f\"‚úÖ Modelo salvo: xgb_{coluna}_t{passo}.joblib\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda3c5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Treinando XGBoost para D3 | t+1 (look_back=5)\n",
      "‚úÖ Modelo salvo: xgb_D3_t1.joblib\n",
      "üöÄ Treinando XGBoost para D3 | t+3 (look_back=5)\n",
      "‚úÖ Modelo salvo: xgb_D3_t3.joblib\n",
      "üöÄ Treinando XGBoost para D3 | t+5 (look_back=5)\n",
      "‚úÖ Modelo salvo: xgb_D3_t5.joblib\n",
      "üöÄ Treinando XGBoost para D3 | t+7 (look_back=10)\n",
      "‚úÖ Modelo salvo: xgb_D3_t7.joblib\n",
      "üöÄ Treinando XGBoost para D3 | t+30 (look_back=10)\n",
      "‚úÖ Modelo salvo: xgb_D3_t30.joblib\n",
      "üöÄ Treinando XGBoost para D2 | t+1 (look_back=5)\n",
      "‚úÖ Modelo salvo: xgb_D2_t1.joblib\n",
      "üöÄ Treinando XGBoost para D2 | t+3 (look_back=5)\n",
      "‚úÖ Modelo salvo: xgb_D2_t3.joblib\n",
      "üöÄ Treinando XGBoost para D2 | t+5 (look_back=5)\n",
      "‚úÖ Modelo salvo: xgb_D2_t5.joblib\n",
      "üöÄ Treinando XGBoost para D2 | t+7 (look_back=10)\n",
      "‚úÖ Modelo salvo: xgb_D2_t7.joblib\n",
      "üöÄ Treinando XGBoost para D2 | t+30 (look_back=10)\n",
      "‚úÖ Modelo salvo: xgb_D2_t30.joblib\n",
      "üöÄ Treinando XGBoost para D1 | t+1 (look_back=5)\n",
      "‚úÖ Modelo salvo: xgb_D1_t1.joblib\n",
      "üöÄ Treinando XGBoost para D1 | t+3 (look_back=5)\n",
      "‚úÖ Modelo salvo: xgb_D1_t3.joblib\n",
      "üöÄ Treinando XGBoost para D1 | t+5 (look_back=5)\n",
      "‚úÖ Modelo salvo: xgb_D1_t5.joblib\n",
      "üöÄ Treinando XGBoost para D1 | t+7 (look_back=10)\n",
      "‚úÖ Modelo salvo: xgb_D1_t7.joblib\n",
      "üöÄ Treinando XGBoost para D1 | t+30 (look_back=10)\n",
      "‚úÖ Modelo salvo: xgb_D1_t30.joblib\n"
     ]
    }
   ],
   "source": [
    "treinar_xgb_multiplos_passos(\"D3_component.csv\", \"D3\")\n",
    "treinar_xgb_multiplos_passos(\"D2_component.csv\", \"D2\")\n",
    "treinar_xgb_multiplos_passos(\"D1_component.csv\", \"D1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8fc249",
   "metadata": {},
   "source": [
    "## Treinar com ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a15f4179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Fun√ß√£o para criar dataset multi-step\n",
    "def criar_dataset_multi_step(series, look_back=10, passo=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(series) - look_back - passo + 1):\n",
    "        X.append(series[i:i+look_back])\n",
    "        y.append(series[i+look_back+passo-1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Fun√ß√£o para treinar ExtraTrees para m√∫ltiplos passos\n",
    "def treinar_etr_multiplos_passos(caminho_csv, coluna, passos=[1, 3, 5, 7, 30]):\n",
    "    dados = pd.read_csv(caminho_csv)[coluna].values\n",
    "    dados = dados.reshape(-1)  # j√° normalizado externamente\n",
    "\n",
    "    for passo in passos:\n",
    "        look_back = 5 if passo in [1, 3, 5] else 10\n",
    "\n",
    "        print(f\"üå≤ Treinando ExtraTrees para {coluna} | t+{passo} (look_back={look_back})\")\n",
    "\n",
    "        X, y = criar_dataset_multi_step(dados, look_back=look_back, passo=passo)\n",
    "\n",
    "        split = int(0.7 * len(X))\n",
    "        X_train, X_test = X[:split], X[split:]\n",
    "        y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "        model = ExtraTreesRegressor()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Salvar modelo\n",
    "        joblib.dump(model, f\"etr_{coluna}_t{passo}.joblib\")\n",
    "        print(f\"‚úÖ Modelo salvo: etr_{coluna}_t{passo}.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "620503fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå≤ Treinando ExtraTrees para D3 | t+1 (look_back=5)\n",
      "‚úÖ Modelo salvo: etr_D3_t1.joblib\n",
      "üå≤ Treinando ExtraTrees para D3 | t+3 (look_back=5)\n",
      "‚úÖ Modelo salvo: etr_D3_t3.joblib\n",
      "üå≤ Treinando ExtraTrees para D3 | t+5 (look_back=5)\n",
      "‚úÖ Modelo salvo: etr_D3_t5.joblib\n",
      "üå≤ Treinando ExtraTrees para D3 | t+7 (look_back=10)\n",
      "‚úÖ Modelo salvo: etr_D3_t7.joblib\n",
      "üå≤ Treinando ExtraTrees para D3 | t+30 (look_back=10)\n",
      "‚úÖ Modelo salvo: etr_D3_t30.joblib\n",
      "üå≤ Treinando ExtraTrees para D2 | t+1 (look_back=5)\n",
      "‚úÖ Modelo salvo: etr_D2_t1.joblib\n",
      "üå≤ Treinando ExtraTrees para D2 | t+3 (look_back=5)\n",
      "‚úÖ Modelo salvo: etr_D2_t3.joblib\n",
      "üå≤ Treinando ExtraTrees para D2 | t+5 (look_back=5)\n",
      "‚úÖ Modelo salvo: etr_D2_t5.joblib\n",
      "üå≤ Treinando ExtraTrees para D2 | t+7 (look_back=10)\n",
      "‚úÖ Modelo salvo: etr_D2_t7.joblib\n",
      "üå≤ Treinando ExtraTrees para D2 | t+30 (look_back=10)\n",
      "‚úÖ Modelo salvo: etr_D2_t30.joblib\n",
      "üå≤ Treinando ExtraTrees para D1 | t+1 (look_back=5)\n",
      "‚úÖ Modelo salvo: etr_D1_t1.joblib\n",
      "üå≤ Treinando ExtraTrees para D1 | t+3 (look_back=5)\n",
      "‚úÖ Modelo salvo: etr_D1_t3.joblib\n",
      "üå≤ Treinando ExtraTrees para D1 | t+5 (look_back=5)\n",
      "‚úÖ Modelo salvo: etr_D1_t5.joblib\n",
      "üå≤ Treinando ExtraTrees para D1 | t+7 (look_back=10)\n",
      "‚úÖ Modelo salvo: etr_D1_t7.joblib\n",
      "üå≤ Treinando ExtraTrees para D1 | t+30 (look_back=10)\n",
      "‚úÖ Modelo salvo: etr_D1_t30.joblib\n"
     ]
    }
   ],
   "source": [
    "treinar_etr_multiplos_passos(\"D3_component.csv\", \"D3\")\n",
    "treinar_etr_multiplos_passos(\"D2_component.csv\", \"D2\")\n",
    "treinar_etr_multiplos_passos(\"D1_component.csv\", \"D1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213aa333",
   "metadata": {},
   "source": [
    "## Treinando para LigthGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "944bef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "def criar_dataset_multi_step(series, look_back=10, passo=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(series) - look_back - passo + 1):\n",
    "        X.append(series[i:i+look_back])\n",
    "        y.append(series[i+look_back+passo-1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def treinar_lgbm_multiplos_passos(caminho_csv, coluna, passos=[1, 3, 5, 7, 30]):\n",
    "    dados = pd.read_csv(caminho_csv)[coluna].values\n",
    "    dados = dados.reshape(-1)\n",
    "\n",
    "    for passo in passos:\n",
    "        look_back = 5 if passo in [1, 3, 5] else 10\n",
    "\n",
    "        print(f\"üí° Treinando LightGBM para {coluna} | t+{passo} (look_back={look_back})\")\n",
    "        X, y = criar_dataset_multi_step(dados, look_back=look_back, passo=passo)\n",
    "\n",
    "        split = int(0.7 * len(X))\n",
    "        X_train, X_test = X[:split], X[split:]\n",
    "        y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "        model = LGBMRegressor(min_data_in_leaf=3,min_split_gain=0.0001,verbose=-1)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        joblib.dump(model, f\"lgbm_{coluna}_t{passo}.joblib\")\n",
    "        print(f\"‚úÖ Modelo salvo: lgbm_{coluna}_t{passo}.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22a0acd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí° Treinando LightGBM para D1 | t+1 (look_back=5)\n",
      "‚úÖ Modelo salvo: lgbm_D1_t1.joblib\n",
      "üí° Treinando LightGBM para D1 | t+3 (look_back=5)\n",
      "‚úÖ Modelo salvo: lgbm_D1_t3.joblib\n",
      "üí° Treinando LightGBM para D1 | t+5 (look_back=5)\n",
      "‚úÖ Modelo salvo: lgbm_D1_t5.joblib\n",
      "üí° Treinando LightGBM para D1 | t+7 (look_back=10)\n",
      "‚úÖ Modelo salvo: lgbm_D1_t7.joblib\n",
      "üí° Treinando LightGBM para D1 | t+30 (look_back=10)\n",
      "‚úÖ Modelo salvo: lgbm_D1_t30.joblib\n",
      "üí° Treinando LightGBM para D2 | t+1 (look_back=5)\n",
      "‚úÖ Modelo salvo: lgbm_D2_t1.joblib\n",
      "üí° Treinando LightGBM para D2 | t+3 (look_back=5)\n",
      "‚úÖ Modelo salvo: lgbm_D2_t3.joblib\n",
      "üí° Treinando LightGBM para D2 | t+5 (look_back=5)\n",
      "‚úÖ Modelo salvo: lgbm_D2_t5.joblib\n",
      "üí° Treinando LightGBM para D2 | t+7 (look_back=10)\n",
      "‚úÖ Modelo salvo: lgbm_D2_t7.joblib\n",
      "üí° Treinando LightGBM para D2 | t+30 (look_back=10)\n",
      "‚úÖ Modelo salvo: lgbm_D2_t30.joblib\n",
      "üí° Treinando LightGBM para D3 | t+1 (look_back=5)\n",
      "‚úÖ Modelo salvo: lgbm_D3_t1.joblib\n",
      "üí° Treinando LightGBM para D3 | t+3 (look_back=5)\n",
      "‚úÖ Modelo salvo: lgbm_D3_t3.joblib\n",
      "üí° Treinando LightGBM para D3 | t+5 (look_back=5)\n",
      "‚úÖ Modelo salvo: lgbm_D3_t5.joblib\n",
      "üí° Treinando LightGBM para D3 | t+7 (look_back=10)\n",
      "‚úÖ Modelo salvo: lgbm_D3_t7.joblib\n",
      "üí° Treinando LightGBM para D3 | t+30 (look_back=10)\n",
      "‚úÖ Modelo salvo: lgbm_D3_t30.joblib\n"
     ]
    }
   ],
   "source": [
    "treinar_lgbm_multiplos_passos(\"D1_component.csv\", \"D1\")\n",
    "treinar_lgbm_multiplos_passos(\"D2_component.csv\", \"D2\")\n",
    "treinar_lgbm_multiplos_passos(\"D3_component.csv\", \"D3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f750bc",
   "metadata": {},
   "source": [
    "## Treinando com CatBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74c3cc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "def treinar_catboost_multiplos_passos(caminho_csv, coluna, passos=[1, 3, 5, 7, 30]):\n",
    "    dados = pd.read_csv(caminho_csv)[coluna].values\n",
    "    dados = dados.reshape(-1)\n",
    "\n",
    "    for passo in passos:\n",
    "        look_back = 5 if passo in [1, 3, 5] else 10\n",
    "\n",
    "        print(f\"üê± Treinando CatBoost para {coluna} | t+{passo} (look_back={look_back})\")\n",
    "        X, y = criar_dataset_multi_step(dados, look_back=look_back, passo=passo)\n",
    "\n",
    "        split = int(0.7 * len(X))\n",
    "        X_train, X_test = X[:split], X[split:]\n",
    "        y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "        model = CatBoostRegressor(verbose=0)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        joblib.dump(model, f\"catboost_{coluna}_t{passo}.joblib\")\n",
    "        print(f\"‚úÖ Modelo salvo: catboost_{coluna}_t{passo}.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a785c9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üê± Treinando CatBoost para D1 | t+1 (look_back=5)\n",
      "‚úÖ Modelo salvo: catboost_D1_t1.joblib\n",
      "üê± Treinando CatBoost para D1 | t+3 (look_back=5)\n",
      "‚úÖ Modelo salvo: catboost_D1_t3.joblib\n",
      "üê± Treinando CatBoost para D1 | t+5 (look_back=5)\n",
      "‚úÖ Modelo salvo: catboost_D1_t5.joblib\n",
      "üê± Treinando CatBoost para D1 | t+7 (look_back=10)\n",
      "‚úÖ Modelo salvo: catboost_D1_t7.joblib\n",
      "üê± Treinando CatBoost para D1 | t+30 (look_back=10)\n",
      "‚úÖ Modelo salvo: catboost_D1_t30.joblib\n",
      "üê± Treinando CatBoost para D2 | t+1 (look_back=5)\n",
      "‚úÖ Modelo salvo: catboost_D2_t1.joblib\n",
      "üê± Treinando CatBoost para D2 | t+3 (look_back=5)\n",
      "‚úÖ Modelo salvo: catboost_D2_t3.joblib\n",
      "üê± Treinando CatBoost para D2 | t+5 (look_back=5)\n",
      "‚úÖ Modelo salvo: catboost_D2_t5.joblib\n",
      "üê± Treinando CatBoost para D2 | t+7 (look_back=10)\n",
      "‚úÖ Modelo salvo: catboost_D2_t7.joblib\n",
      "üê± Treinando CatBoost para D2 | t+30 (look_back=10)\n",
      "‚úÖ Modelo salvo: catboost_D2_t30.joblib\n",
      "üê± Treinando CatBoost para D3 | t+1 (look_back=5)\n",
      "‚úÖ Modelo salvo: catboost_D3_t1.joblib\n",
      "üê± Treinando CatBoost para D3 | t+3 (look_back=5)\n",
      "‚úÖ Modelo salvo: catboost_D3_t3.joblib\n",
      "üê± Treinando CatBoost para D3 | t+5 (look_back=5)\n",
      "‚úÖ Modelo salvo: catboost_D3_t5.joblib\n",
      "üê± Treinando CatBoost para D3 | t+7 (look_back=10)\n",
      "‚úÖ Modelo salvo: catboost_D3_t7.joblib\n",
      "üê± Treinando CatBoost para D3 | t+30 (look_back=10)\n",
      "‚úÖ Modelo salvo: catboost_D3_t30.joblib\n"
     ]
    }
   ],
   "source": [
    "treinar_catboost_multiplos_passos(\"D1_component.csv\", \"D1\")\n",
    "treinar_catboost_multiplos_passos(\"D2_component.csv\", \"D2\")\n",
    "treinar_catboost_multiplos_passos(\"D3_component.csv\", \"D3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7072303",
   "metadata": {},
   "source": [
    "## Usar uma MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a1f2e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Fun√ß√£o para criar dataset com sa√≠da m√©dia dos pr√≥ximos `passo` valores\n",
    "def criar_dataset_com_media_futura(series, look_back=5, passo=3):\n",
    "    X, y = [], []\n",
    "    for i in range(len(series) - look_back - passo + 1):\n",
    "        entrada = series[i:i+look_back]\n",
    "        janela_futura = series[i+look_back:i+look_back+passo]\n",
    "        media_futura = np.mean(janela_futura)\n",
    "        X.append(entrada)\n",
    "        y.append(media_futura)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Fun√ß√£o para treinar MLP com sa√≠da m√©dia futura\n",
    "def treinar_mlp_media_futura(caminho_csv, coluna, passos=[1, 3, 5, 7, 30]):\n",
    "    dados = pd.read_csv(caminho_csv)[coluna].values\n",
    "    dados = dados.reshape(-1)\n",
    "\n",
    "    for passo in passos:\n",
    "        look_back = 5 if passo in [1, 3, 5] else 10\n",
    "        print(f\"üîÅ Treinando MLP para m√©dia futura de {coluna} | passo={passo} (look_back={look_back})\")\n",
    "\n",
    "        X, y = criar_dataset_com_media_futura(dados, look_back=look_back, passo=passo)\n",
    "\n",
    "        split = int(0.7 * len(X))\n",
    "        X_train, X_test = X[:split], X[split:]\n",
    "        y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "        model = MLPRegressor(\n",
    "                                hidden_layer_sizes=(150, 100, 50),\n",
    "                                activation='relu',\n",
    "                                solver='adam',\n",
    "                                learning_rate='adaptive',\n",
    "                                max_iter=1000,\n",
    "                                early_stopping=True,\n",
    "                                validation_fraction=0.1,\n",
    "                                n_iter_no_change=20\n",
    "                        )\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        joblib.dump(model, f\"mlpavg_{coluna}_t{passo}.joblib\")\n",
    "        print(f\"‚úÖ Modelo salvo: mlpavg_{coluna}_t{passo}.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b21ec12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Treinando MLP para m√©dia futura de D1 | passo=1 (look_back=5)\n",
      "‚úÖ Modelo salvo: mlpavg_D1_t1.joblib\n",
      "üîÅ Treinando MLP para m√©dia futura de D1 | passo=3 (look_back=5)\n",
      "‚úÖ Modelo salvo: mlpavg_D1_t3.joblib\n",
      "üîÅ Treinando MLP para m√©dia futura de D1 | passo=5 (look_back=5)\n",
      "‚úÖ Modelo salvo: mlpavg_D1_t5.joblib\n",
      "üîÅ Treinando MLP para m√©dia futura de D1 | passo=7 (look_back=10)\n",
      "‚úÖ Modelo salvo: mlpavg_D1_t7.joblib\n",
      "üîÅ Treinando MLP para m√©dia futura de D1 | passo=30 (look_back=10)\n",
      "‚úÖ Modelo salvo: mlpavg_D1_t30.joblib\n",
      "üîÅ Treinando MLP para m√©dia futura de D2 | passo=1 (look_back=5)\n",
      "‚úÖ Modelo salvo: mlpavg_D2_t1.joblib\n",
      "üîÅ Treinando MLP para m√©dia futura de D2 | passo=3 (look_back=5)\n",
      "‚úÖ Modelo salvo: mlpavg_D2_t3.joblib\n",
      "üîÅ Treinando MLP para m√©dia futura de D2 | passo=5 (look_back=5)\n",
      "‚úÖ Modelo salvo: mlpavg_D2_t5.joblib\n",
      "üîÅ Treinando MLP para m√©dia futura de D2 | passo=7 (look_back=10)\n",
      "‚úÖ Modelo salvo: mlpavg_D2_t7.joblib\n",
      "üîÅ Treinando MLP para m√©dia futura de D2 | passo=30 (look_back=10)\n",
      "‚úÖ Modelo salvo: mlpavg_D2_t30.joblib\n",
      "üîÅ Treinando MLP para m√©dia futura de D3 | passo=1 (look_back=5)\n",
      "‚úÖ Modelo salvo: mlpavg_D3_t1.joblib\n",
      "üîÅ Treinando MLP para m√©dia futura de D3 | passo=3 (look_back=5)\n",
      "‚úÖ Modelo salvo: mlpavg_D3_t3.joblib\n",
      "üîÅ Treinando MLP para m√©dia futura de D3 | passo=5 (look_back=5)\n",
      "‚úÖ Modelo salvo: mlpavg_D3_t5.joblib\n",
      "üîÅ Treinando MLP para m√©dia futura de D3 | passo=7 (look_back=10)\n",
      "‚úÖ Modelo salvo: mlpavg_D3_t7.joblib\n",
      "üîÅ Treinando MLP para m√©dia futura de D3 | passo=30 (look_back=10)\n",
      "‚úÖ Modelo salvo: mlpavg_D3_t30.joblib\n"
     ]
    }
   ],
   "source": [
    "treinar_mlp_media_futura(\"D1_component.csv\", \"D1\")\n",
    "treinar_mlp_media_futura(\"D2_component.csv\", \"D2\")\n",
    "treinar_mlp_media_futura(\"D3_component.csv\", \"D3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b1284c",
   "metadata": {},
   "source": [
    "## Criando uam MLP refinada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce923d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywt\n",
    "import joblib\n",
    "\n",
    "def criar_dataset_media(series, look_back=10, passo=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(series) - look_back - passo + 1):\n",
    "        entrada = series[i:i+look_back]\n",
    "        janela_futura = series[i+look_back:i+look_back+passo]\n",
    "        media_futura = np.mean(janela_futura)\n",
    "        X.append(entrada)\n",
    "        y.append(media_futura)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def treinar_mlp_para_a3(path_csv_norm, passos=[1, 3, 5, 7, 30]):\n",
    "    df = pd.read_csv(path_csv_norm)\n",
    "    serie_norm = df[\"RMDM_Norm\"].values\n",
    "    coeffs = pywt.wavedec(serie_norm, 'db4', level=3)\n",
    "    a3 = pywt.upcoef('a', coeffs[0], 'db4', level=3)[:len(serie_norm)]\n",
    "\n",
    "    for passo in passos:\n",
    "        look_back = 5 if passo in [1, 3, 5] else 10\n",
    "        print(f\"üîÅ Treinando MLP para A3 | t+{passo} (look_back={look_back})\")\n",
    "\n",
    "        X, y = criar_dataset_media(a3, look_back=look_back, passo=passo)\n",
    "\n",
    "        # Remo√ß√£o de NaNs (precau√ß√£o)\n",
    "        mask = ~np.isnan(X).any(axis=1) & ~np.isnan(y)\n",
    "        X, y = X[mask], y[mask]\n",
    "\n",
    "        model = MLPRegressor(\n",
    "            hidden_layer_sizes=(200, 150, 100, 50),  # üîº Mais capacidade\n",
    "            activation='relu',\n",
    "            solver='adam',\n",
    "            alpha=0.0005, # üîß Regulariza√ß√£o L2\n",
    "            learning_rate='adaptive',\n",
    "            learning_rate_init=0.001,\n",
    "            early_stopping=True,\n",
    "            validation_fraction=0.1,\n",
    "            n_iter_no_change=30,\n",
    "            max_iter=2000,\n",
    "            random_state=42)\n",
    "        model.fit(X, y)\n",
    "        joblib.dump(model, f\"mlp_a3_t{passo}.joblib\")\n",
    "        print(f\"‚úÖ Modelo salvo: mlp_a3_t{passo}.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "392afd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Treinando MLP para A3 | t+1 (look_back=5)\n",
      "‚úÖ Modelo salvo: mlp_a3_t1.joblib\n",
      "üîÅ Treinando MLP para A3 | t+3 (look_back=5)\n",
      "‚úÖ Modelo salvo: mlp_a3_t3.joblib\n",
      "üîÅ Treinando MLP para A3 | t+5 (look_back=5)\n",
      "‚úÖ Modelo salvo: mlp_a3_t5.joblib\n",
      "üîÅ Treinando MLP para A3 | t+7 (look_back=10)\n",
      "‚úÖ Modelo salvo: mlp_a3_t7.joblib\n",
      "üîÅ Treinando MLP para A3 | t+30 (look_back=10)\n",
      "‚úÖ Modelo salvo: mlp_a3_t30.joblib\n"
     ]
    }
   ],
   "source": [
    "treinar_mlp_para_a3(\"RMDM_litoral_norte_com_normalizado.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723d4238",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eade96bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
